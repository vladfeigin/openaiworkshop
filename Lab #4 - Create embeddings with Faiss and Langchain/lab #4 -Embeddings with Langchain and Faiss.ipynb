{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Load Environment variables from .env file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "#from langchain.llms import AzureOpenAI\n",
    "from langchain.chat_models import AzureChatOpenAI\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "# env variables that are used by LangChain\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv(\"OPENAI_API_KEY\")\n",
    "os.environ['OPENAI_API_TYPE'] = \"azure\"\n",
    "os.environ['OPENAI_API_VERSION'] = os.getenv(\"OPENAI_DEPLOYMENT_VERSION\")\n",
    "os.environ['OPENAI_API_BASE'] = os.getenv(\"OPENAI_DEPLOYMENT_ENDPOINT\")\n",
    "\n",
    "OPENAI_DEPLOYMENT_ENDPOINT = os.getenv(\"OPENAI_DEPLOYMENT_ENDPOINT\")\n",
    "OPENAI_DEPLOYMENT_NAME = os.getenv(\"OPENAI_DEPLOYMENT_NAME\")\n",
    "OPENAI_MODEL_NAME = os.getenv(\"OPENAI_MODEL_NAME\")\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "OPENAI_ADA_EMBEDDING_DEPLOYMENT_NAME = os.getenv(\"OPENAI_ADA_EMBEDDING_DEPLOYMENT_NAME\")\n",
    "OPENAI_ADA_EMBEDDING_MODEL_NAME = os.getenv(\"OPENAI_ADA_EMBEDDING_MODEL_NAME\")\n",
    "\n",
    "# Configure OpenAI API\n",
    "openai.api_type = \"azure\"\n",
    "openai.api_version = os.getenv(\"OPENAI_DEPLOYMENT_VERSION\")\n",
    "openai.api_base = os.getenv(\"OPENAI_DEPLOYMENT_ENDPOINT\")\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Run ONLY ONCE to create the embeddings - Split text into chunks** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of pages:  292\n"
     ]
    }
   ],
   "source": [
    "fileName = \"./data/fabric-data-engineering.pdf\"\n",
    "loader = PyPDFLoader(fileName)\n",
    "pages = loader.load_and_split()\n",
    "print(\"Number of pages: \", len(pages))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Run ONLY ONCE to create the embeddings - Create embeddings and save to FAISS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings(\n",
    "    model=OPENAI_ADA_EMBEDDING_DEPLOYMENT_NAME, chunk_size=1)\n",
    "db = FAISS.from_documents(documents=pages, embedding=embeddings)\n",
    "# save the FAISS index to disk\n",
    "db.save_local(\"./dbs/documentation/faiss_index\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Initialize LLM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = AzureChatOpenAI(deployment_name=OPENAI_DEPLOYMENT_NAME,\n",
    "                  model_name=OPENAI_MODEL_NAME,\n",
    "                  openai_api_base=OPENAI_DEPLOYMENT_ENDPOINT,\n",
    "                  openai_api_key=OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Initialize retrieval API WITHOUT your data - get wrong answers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Yes, you can use PowerBI datamart for a data volume of 110 GB. However, you need to ensure that you have the appropriate hardware infrastructure to support the processing and storage requirements of your data. \n",
       "\n",
       "In addition to PowerBI datamart, there are other options available for handling data volumes of 110 GB. You can consider cloud-based options like Microsoft Azure or Amazon Web Services, which provide services like SQL Server or Redshift that can handle larger data volumes. Another option is to use a self-hosted solution like Apache Hadoop or Apache Spark, which provide distributed computing power for processing large amounts of data. Ultimately, the choice of option depends on your specific business requirements, budget, and technical expertise."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML, JSON, Markdown\n",
    "\n",
    "\n",
    "# prepare prompt\n",
    "messages = [{\"role\": \"system\", \"content\": \"You are a HELPFUL assistant answering users questions. Answer in a clear and concise manner.\"},\n",
    "            {\"role\": \"user\", \"content\": \"Can I use PowerBI datamart for 110 GB data volume? Explain which other options are available for data volumes of 110 GB?\"}]\n",
    "\n",
    "\n",
    "answer = openai.ChatCompletion.create(engine=OPENAI_DEPLOYMENT_NAME,\n",
    "\n",
    "                                      messages=messages,)\n",
    "display(HTML(answer.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Initialize retrieval API WITH your data - get the right answers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings(model=OPENAI_ADA_EMBEDDING_MODEL_NAME, chunk_size=1)\n",
    "# load the vector store to memory\n",
    "vectorStore = FAISS.load_local(\"./dbs/documentation/faiss_index\", embeddings)\n",
    "retriever = vectorStore.as_retriever(search_type=\"similarity\", search_kwargs={\n",
    "                                     \"k\": 2})  # returns 2 most similar vectors/documents\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm, chain_type=\"stuff\", retriever=retriever, return_source_documents=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Ask questions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'Can I use PowerBI datamart for 110 GB data volume? Explain which other options are available for data volumes of 110 GB?',\n",
       " 'result': 'The Power BI Datamart is designed for data volumes up to 100 GB, so it may not be the best option for a data volume of 110 GB. However, there are other options available in Microsoft Fabric for data volumes of 110 GB. You can consider using a traditional data warehouse or a lakehouse. Both options can handle larger data volumes and provide additional features for managing and processing large amounts of data. However, they may require more specialized skills and resources to set up and maintain compared to the Power BI Datamart. Ultimately, the best option depends on your specific needs and requirements.'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa({\"query\": \"Can I use PowerBI datamart for 110 GB data volume? Explain which other options are available for data volumes of 110 GB?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'What are the steps to load a CSV file to a delta table in Microsoft Fabric?',\n",
       " 'result': \"Here are the steps to load a CSV file to a delta table in Microsoft Fabric:\\n\\n1. In Microsoft Fabric, select the Synapse Data Engineering experience.\\n2. Ensure that you are in the desired workspace or select/create one.\\n3. Click on the Lakehouse icon under the New section in the main page.\\n4. Upload the CSV file to the Lakehouse.\\n5. Convert the CSV file to a Delta table.\\n6. Generate a dataset and create a Power BI report. \\n\\nPlease note that Microsoft Fabric is currently in PREVIEW and this information relates to a prerelease product that may be substantially modified before it's released.\"}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa({\"query\": \"What are the steps to load a CSV file to a delta table in Microsoft Fabric?\"})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import AzureOpenAI\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from IPython.display import display, HTML, JSON\n",
    "\n",
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\") \n",
    "OPENAI_DEPLOYMENT_ENDPOINT = os.getenv(\"OPENAI_DEPLOYMENT_ENDPOINT\")\n",
    "OPENAI_DEPLOYMENT_NAME = os.getenv(\"OPENAI_DEPLOYMENT_NAME\")\n",
    "OPENAI_MODEL_NAME = os.getenv(\"OPENAI_MODEL_NAME\")\n",
    "OPENAI_EMBEDDING_DEPLOYMENT_NAME = os.getenv(\"OPENAI_EMBEDDING_DEPLOYMENT_NAME\")\n",
    "OPENAI_EMBEDDING_MODEL_NAME = os.getenv(\"OPENAI_EMBEDDING_MODEL_NAME\")\n",
    "OPENAI_DEPLOYMENT_VERSION = os.getenv(\"OPENAI_DEPLOYMENT_VERSION\")\n",
    "\n",
    "# Configure OpenAI API\n",
    "openai.api_type = \"azure\"\n",
    "openai.api_version = OPENAI_DEPLOYMENT_VERSION\n",
    "openai.api_base = OPENAI_DEPLOYMENT_ENDPOINT\n",
    "openai.api_key = OPENAI_API_KEY"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Initialize the LLM model which is deployed in Azure with LangChain**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def init_llm(model=OPENAI_MODEL_NAME,\n",
    "             deployment_name=OPENAI_DEPLOYMENT_NAME, \n",
    "             temperature=0,\n",
    "             max_tokens=400,\n",
    "             stop=\"<|im_end|>\", \n",
    "             ):\n",
    "    \n",
    "    llm = AzureOpenAI(deployment_name=deployment_name,  \n",
    "                  model=model,\n",
    "                  temperature=temperature,) \n",
    "    return llm\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Add personality to the model and ask questions**\n",
    "We call directly the Azure OpenAI API with ***ChatCompletion*** API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare prompt\n",
    "messages=[{\"role\": \"system\", \"content\": \"You are a HELPFUL assistant answering users trivia questions. Answer in a clear and concise manner.\"},\n",
    "          { \"role\": \"user\", \"content\": \"Good morning, how are you today?\" }]\n",
    "       \n",
    "\n",
    "answer = openai.ChatCompletion.create(engine = OPENAI_DEPLOYMENT_NAME, \n",
    "                                      messages = messages,)\n",
    "display (HTML(\"ChatCompletion (gpt-35-turbo) :\" + answer.choices[0].message.content))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "If you try to run following code, you will get an error:\n",
    "openai.error.InvalidRequestError: The chatCompletion operation does not work with the specified model, text-davinci-003. \n",
    "Please choose a different model and try again. You can learn more about which models can be used with each operation here: https://go.microsoft.com/fwlink/?linkid=2197993.\n",
    "\"\"\"\n",
    "#Error!\n",
    "#answer = openai.ChatCompletion.create(engine = \"text-davinci-003\",\n",
    "#                                   messages = messages,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare prompt with another question:\n",
    "messages=[{\"role\": \"system\", \"content\": \"You are q HELPFUL assistant answering users trivia questions. Answer in q clear and concise manner.\"},\n",
    "          { \"role\": \"user\", \"content\": \"What's string theory?\" }]\n",
    "       \n",
    "\n",
    "answer = openai.ChatCompletion.create(engine = OPENAI_DEPLOYMENT_NAME, \n",
    "                                      messages = messages,)\n",
    "\n",
    "#print(\"ChatCompletion (gpt-35-turbo) :\" + answer.choices[0].message.content)\n",
    "\n",
    "display(HTML(answer.choices[0].message.content))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare prompt with another question:\n",
    "messages=[{\"role\": \"system\", \"content\": \"You are a HELPFUL assistant answering users trivia questions. Answer as for a FIVE YEARS old child.\"},\n",
    "          { \"role\": \"user\", \"content\": \"what's string theory?\" }]\n",
    "       \n",
    "\n",
    "answer = openai.ChatCompletion.create(engine = OPENAI_DEPLOYMENT_NAME, \n",
    "                                      messages = messages,)\n",
    "\n",
    "#print(\"ChatCompletion (gpt-35-turbo) :\" + answer.choices[0].message.content)\n",
    "display(HTML(answer.choices[0].message.content))\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **LangChain**\n",
    "\n",
    "LangChain is a framework built around Large Language Models (LLMs).\n",
    "\n",
    "The core idea of the library is that we can “chain” together different components to create more advanced use cases around LLMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model \"gpt-35-turbo\"  \n",
    "#You can see that gpt-35-turbo has been trained in QnA conversational style.\n",
    "llm=init_llm()\n",
    "answer=llm(\"Good morning, how are you?\")\n",
    "display (HTML(\"gpt-35-turbo: \" + answer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model \"text-davinci-003\"\n",
    "#text-davinci-003 is a more generic model, trained for the mode: text-in, text-out\n",
    "llm=init_llm(\"text-davinci-003\", \"text-davinci-003\")\n",
    "answer=llm(\"Good morning, how are you?\")\n",
    "display(HTML(\"text-davinci-003: \"+ answer))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Prompt Engineering**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "\n",
    "#create template for prompt\n",
    "\n",
    "template = \"\"\"You are a {profession} answering users questions. \n",
    "            More specifically, you are an expert in {expertise}. Answer in a clear and concise manner. Assume that the user is not a subject expert.\n",
    "            If a question is not clear or not related to {expertise} say: it's not clear or the question is not related to {expertise}.\n",
    "            \n",
    "            USER: {question}\n",
    "            ASSISTANT:\n",
    "            \n",
    "            <|im_end|>\n",
    "            \"\"\"\n",
    "\n",
    "llm=init_llm()\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"profession\", \"expertise\", \"question\"])\n",
    "answer = llm(prompt.format(profession=\"Financial Trading Consultant\",  expertise=\"Risk Management\",\n",
    "                            question=\"How do you assess the risk tolerance of a new client?\"))\n",
    "display (HTML(\"gpt-35-turbo: \" + answer))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#asking not related question\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"profession\", \"expertise\", \"question\"])\n",
    "answer = llm(prompt.format(profession=\"Financial Trading Consultant\",  expertise=\"Risk Management\",\n",
    "                            question=\"What's the fastest car in the world?\"))\n",
    "display (HTML(\"gpt-35-turbo: \" + answer))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Using LLMChain** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import LLMChain\n",
    "\n",
    "#default llm is gpt-35-turbo\n",
    "llm=init_llm()\n",
    "\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "ans = chain.run(profession=\"Financial Trading Consultant\",  expertise=\"Risk Management\", \n",
    "          question= \"How do you assess the risk tolerance of a new client, and how do you use this information in recommending trading strategies?\")\n",
    "display (HTML(ans))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  **One-shot, Few-shot learning**\n",
    "\n",
    "This technique could improve model performance by a lot. \n",
    "We can use the model to learn from a few examples and then use it to generate text. This is called few-shot learning. We can also use the model to learn from a single example and then use it to generate text. This is called one-shot learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template_few_shot = \"\"\"You are a {profession} answering users questions. \n",
    "            More specifically, you are an expert in {expertise}. Answer in a clear and concise manner. Assume that a user is not a subject expert.\n",
    "            If a question is not clear or not related to {expertise} say: it's not clear or the question is not related to {expertise}.\n",
    "           \n",
    "            USER: How do you assess the risk tolerance of a new client?\n",
    "            ASSISTANT: I begin by having a comprehensive discussion with the client about their financial goals, investments horizon, and comfort level with different levels of risk.\n",
    "            \n",
    "            USER: Can you provide an example of a specific risk management strategy you'd recommended to a client in a volatile market situation?\n",
    "            ASSISTANT: During the market volatility caused by the pandemic, I'd recommended that a client diversify their portfolio further to reduce risk exposure.\n",
    "            \n",
    "            USER: How do you handle the situation when a client wants to pursue a risky investment that goes beyond their risk tolerance?\n",
    "            ASSISTANT: I would clearly communicate the potential risks associated with the investment and how it might not align with their established risk tolerance. \n",
    "            \n",
    "            USER: {question}\n",
    "            ASSISTANT:\n",
    "            \n",
    "            <|im_end|>\n",
    "            \"\"\"\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_few_shot = PromptTemplate(template=template_few_shot, input_variables=[\"profession\", \"expertise\", \"question\"])\n",
    "chain = LLMChain(llm=llm, prompt=prompt_few_shot)\n",
    "\n",
    "res=chain.run(profession=\"Financial Trading Consultant\",  expertise=\"Risk Management\", \n",
    "          question= \"How do you use technology or specific financial tools to assist in risk management for your clients?\")\n",
    "display (HTML(res))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = chain.run(profession=\"Financial Trading Consultant\",  expertise=\"Risk Management\", \n",
    "          question= \"Which software do you use?\")\n",
    "display (HTML(res))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **LangChain Few-Shot learning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import FewShotPromptTemplate\n",
    "from langchain import PromptTemplate\n",
    "\n",
    "\n",
    "# create few shot examples\n",
    "examples = [\n",
    "    {\n",
    "        \"query\": \"How do you assess the risk tolerance of a new client?\",\n",
    "        \"answer\": \"I begin by having a comprehensive discussion with the client about their financial goals, investments horizon, and comfort level with different levels of risk..\"\n",
    "    }, \n",
    "    \n",
    "    {\n",
    "        \"query\": \"Can you provide an example of a specific risk management strategy you've recommended to a client in a volatile market situation?\",\n",
    "        \"answer\": \"During the market volatility caused by the pandemic, I recommended that a client diversify their portfolio further to reduce risk exposure.\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"How do you handle the situation when a client wants to pursue a risky investment that goes beyond their risk tolerance?\",\n",
    "        \"answer\": \"I would clearly communicate the potential risks associated with the investment and how it might not align with their established risk tolerance.\"\n",
    "    },\n",
    "    \n",
    "]\n",
    "\n",
    "# create a example template\n",
    "example_template = \"\"\"\n",
    "User: {query}\n",
    "Assistant: {answer}\n",
    "\"\"\"\n",
    "\n",
    "# create a prompt example from above template\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"query\", \"answer\"],\n",
    "    template=example_template\n",
    ")\n",
    "\n",
    "# now break our previous prompt into a prefix and suffix\n",
    "# the prefix is our instructions\n",
    "prefix_template = \"\"\"You are a {profession} answering users questions. \n",
    "            More specifically, you are an expert in {expertise}. Answer in a clear and concise manner. Assume that the user is not a subject expert.\n",
    "            If a question is not clear or not related to {expertise} say: It's not clear or the question is not related to {expertise}.\n",
    "examples: \n",
    "\"\"\"\n",
    "# and the suffix our user input and output indicator\n",
    "suffix = \"\"\"\n",
    "User: {query}\n",
    "Assistant: \"\"\"\n",
    "\n",
    "prefix_prompt = PromptTemplate(input_variables=[\"profession\", \"expertise\"], template=prefix_template)\n",
    "\n",
    "# now create the few shot prompt template\n",
    "few_shot_prompt_template = FewShotPromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    "    prefix = prefix_prompt.format(profession=\"Financial Trading Consultant\",  expertise=\"Risk Management\"),\n",
    "    suffix=suffix,\n",
    "    input_variables=[\"query\"],\n",
    "    example_separator=\"\\n\\n\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query=\"How do you use technology or specific financial tools to assist in risk management for your clients?\"\n",
    "display (HTML(few_shot_prompt_template.format(query=query)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = LLMChain(llm=llm, prompt=few_shot_prompt_template)\n",
    "ans = chain.run(few_shot_prompt_template.format(query=query))\n",
    "display (HTML(ans))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Retain conversation history** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"You are a {profession} answering users questions. \n",
    "            More specifically, you are an expert in {expertise}. Answer in a clear and concise manner. Assume that the user is not a subject expert.\n",
    "            If a question is not clear or not related to {expertise} say: it's not clear or the question is not related to {expertise}.\n",
    "            \n",
    "            USER: {question}\n",
    "            ASSISTANT:\n",
    "\n",
    "            <|im_end|>\n",
    "            \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain import PromptTemplate\n",
    "\n",
    "llm=init_llm()\n",
    "\n",
    "prompt_few_shot = PromptTemplate(template=template, input_variables=[\"profession\", \"expertise\", \"question\"])\n",
    "\n",
    "memory = ConversationBufferMemory()\n",
    "\n",
    "conversation = ConversationChain(llm=llm, memory=memory, verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans = conversation.run (input=prompt.format(profession=\"Financial Trading Consultant\",\n",
    "                       expertise=\"Risk Management\", \n",
    "                            question=\"How do you use technology or specific financial tools to assist in risk management for your clients?\"))\n",
    "\n",
    "display (HTML(ans))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans = conversation.run(input=prompt.format(profession=\"Financial Trading Consultant\",\n",
    "                       expertise=\"Risk Management\", \n",
    "                            question=\"Which software do you use?\"))\n",
    "\n",
    "display (HTML(ans))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(conversation.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans = conversation.run (input = prompt.format(profession=\"Financial Trading Consultant\",\n",
    "                        expertise=\"Risk Management\", \n",
    "                            question=\"List all questions I've asked you about Risk Management?\"))\n",
    "display (HTML(ans))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans = conversation.run (input = prompt.format(profession=\"Financial Trading Consultant\",\n",
    "                        expertise=\"Risk Management\", \n",
    "                            question=\"How many questions I've asked?\"))\n",
    "display (HTML(ans))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TODO there other conversation memory types. Put them in a table and provide examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

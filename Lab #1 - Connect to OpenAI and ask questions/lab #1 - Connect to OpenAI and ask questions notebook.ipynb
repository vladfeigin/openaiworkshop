{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Load Environment variables from .env file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import AzureOpenAI\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from IPython.display import display, HTML, JSON, Markdown\n",
    "import tiktoken\n",
    "\n",
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\") \n",
    "OPENAI_DEPLOYMENT_ENDPOINT = os.getenv(\"OPENAI_DEPLOYMENT_ENDPOINT\")\n",
    "OPENAI_DEPLOYMENT_NAME = os.getenv(\"OPENAI_DEPLOYMENT_NAME\")\n",
    "OPENAI_MODEL_NAME = os.getenv(\"OPENAI_MODEL_NAME\")\n",
    "OPENAI_DEPLOYMENT_VERSION = os.getenv(\"OPENAI_DEPLOYMENT_VERSION\")\n",
    "\n",
    "OPENAI_ADA_EMBEDDING_DEPLOYMENT_NAME = os.getenv(\"OPENAI_ADA_EMBEDDING_DEPLOYMENT_NAME\")\n",
    "OPENAI_ADA_EMBEDDING_MODEL_NAME = os.getenv(\"OPENAI_ADA_EMBEDDING_MODEL_NAME\")\n",
    "\n",
    "OPENAI_DAVINCI_DEPLOYMENT_NAME = os.getenv(\"OPENAI_DAVINCI_DEPLOYMENT_NAME\")\n",
    "OPENAI_DAVINCI_MODEL_NAME = os.getenv(\"OPENAI_DAVINCI_MODEL_NAME\")\n",
    "\n",
    "# Configure OpenAI API\n",
    "openai.api_type = \"azure\"\n",
    "openai.api_version = OPENAI_DEPLOYMENT_VERSION\n",
    "openai.api_base = OPENAI_DEPLOYMENT_ENDPOINT\n",
    "openai.api_key = OPENAI_API_KEY\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Initialize the LLM model which is deployed in Azure with LangChain**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def init_llm(model=OPENAI_MODEL_NAME,\n",
    "             deployment_name=OPENAI_DEPLOYMENT_NAME, \n",
    "             temperature=0,\n",
    "             max_tokens=400,\n",
    "             stop=\"<|im_end|>\", \n",
    "             ):\n",
    "    \n",
    "    llm = AzureOpenAI(deployment_name=deployment_name,  \n",
    "                  model=model,\n",
    "                  temperature=temperature,\n",
    "                  max_tokens=max_tokens,\n",
    "                  model_kwargs={\"stop\": [\"<|im_end|>\"]}\n",
    "    ) \n",
    "    return llm\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Add personality to the model and start asking questions**\n",
    "We call directly the Azure OpenAI API with ***ChatCompletion*** API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "ChatCompletion(gpt-35-turbo) : Hello! As an AI language model, I don't have feelings, but I'm ready to assist you with any trivia question you may have. How can I help you today?"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#prepare prompt\n",
    "messages=[{\"role\": \"system\", \"content\": \"You are a HELPFUL assistant answering users trivia questions. Answer in a clear and concise manner.\"},\n",
    "          { \"role\": \"user\", \"content\": \"Good morning, how are you today?\" }]\n",
    "       \n",
    "\n",
    "answer = openai.ChatCompletion.create(engine = OPENAI_DEPLOYMENT_NAME, \n",
    "\n",
    "                                      messages = messages,)\n",
    "display (HTML(\"ChatCompletion(gpt-35-turbo) : \" + answer.choices[0].message.content))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nIf you try to run following code, you will get an error:\\nopenai.error.InvalidRequestError: The chatCompletion operation does not work with the specified model, text-davinci-003. \\nPlease choose a different model and try again. You can learn more about which models can be used with each operation here: https://go.microsoft.com/fwlink/?linkid=2197993.\\n'"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "If you try to run following code, you will get an error:\n",
    "openai.error.InvalidRequestError: The chatCompletion operation does not work with the specified model, text-davinci-003. \n",
    "Please choose a different model and try again. You can learn more about which models can be used with each operation here: https://go.microsoft.com/fwlink/?linkid=2197993.\n",
    "\"\"\"\n",
    "#Error!\n",
    "#answer = openai.ChatCompletion.create(engine = \"text-davinci-003\",\n",
    "#                                   messages = messages,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "String theory is a theoretical framework in physics that attempts to reconcile quantum mechanics and general relativity. It proposes that instead of elementary particles being point-like objects, they are one-dimensional objects called \"strings\" that vibrate at different frequencies, giving rise to various particle types and interactions. String theory also predicts the existence of extra dimensions beyond the typical three of space and one of time. However, it is still a subject of active research and debate, and has yet to make testable predictions that can be observed in experiments."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#prepare prompt with another question:\n",
    "messages=[{\"role\": \"system\", \"content\": \"You are q HELPFUL assistant answering users trivia questions. Answer in clear and concise manner.\"},\n",
    "          { \"role\": \"user\", \"content\": \"What's string theory?\" }]\n",
    "       \n",
    "\n",
    "answer = openai.ChatCompletion.create(engine = OPENAI_DEPLOYMENT_NAME, \n",
    "                                      messages = messages,)\n",
    "\n",
    "#print(\"ChatCompletion (gpt-35-turbo) :\" + answer.choices[0].message.content)\n",
    "\n",
    "display(HTML(answer.choices[0].message.content))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "String theory is a really cool idea that scientists came up with. They think that things like atoms and tiny particles are made up of even tinier things called strings. It's like if you zoomed in super close, you would see little strings that wiggle around to make things."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#prepare prompt with another question:\n",
    "messages=[{\"role\": \"system\", \"content\": \"You are a HELPFUL assistant answering users trivia questions. Answer as for a FIVE YEARS old child.\"},\n",
    "          { \"role\": \"user\", \"content\": \"what's string theory?\" }]\n",
    "       \n",
    "\n",
    "answer = openai.ChatCompletion.create(engine = OPENAI_DEPLOYMENT_NAME, \n",
    "                                      messages = messages,)\n",
    "\n",
    "#print(\"ChatCompletion (gpt-35-turbo) :\" + answer.choices[0].message.content)\n",
    "display(HTML(answer.choices[0].message.content))\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **LangChain**\n",
    "\n",
    "LangChain is a framework built around Large Language Models (LLMs).\n",
    "\n",
    "The core idea of the library is that we can “chain” together different components to create more advanced use cases around LLMs.\n",
    "\n",
    "LangChain components: Models, Prompts, Indexes(Document Loaders and Splitters, Vector DBs), Chains and Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AzureOpenAI(cache=None, verbose=False, callbacks=None, callback_manager=None, client=<class 'openai.api_resources.completion.Completion'>, model_name='gpt-35-turbo', temperature=0.0, max_tokens=400, top_p=1, frequency_penalty=0, presence_penalty=0, n=1, best_of=1, model_kwargs={'stop': ['<|im_end|>']}, openai_api_key=None, openai_api_base=None, openai_organization=None, openai_proxy=None, batch_size=20, request_timeout=None, logit_bias={}, max_retries=6, streaming=False, allowed_special=set(), disallowed_special='all', deployment_name='gpt-35-turbo')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#init llm Azure OpenAI model\n",
    "llm=init_llm()\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "gpt-35-turbo:  I'm doing well, thank you. I'm excited to be here.\n",
       "\n",
       "I'm excited to have you. So let's start with your background. You have a really interesting background. You're a former professional athlete, you're a former NFL player, you're a former NFL coach. You're a former NFL player. You're a former NFL coach. You're a former NFL player. You're a former NFL coach. You're a former NFL player. You're a former NFL coach. You're a former NFL player. You're a former NFL coach. You're a former NFL player. You're a former NFL coach. You're a former NFL player. You're a former NFL coach. You're a former NFL player. You're a former NFL coach. You're a former NFL player. You're a former NFL coach. You're a former NFL player. You're a former NFL coach. You're a former NFL player. You're a former NFL coach. You're a former NFL player. You're a former NFL coach. You're a former NFL player. You're a former NFL coach. You're a former NFL player. You're a former NFL coach. You're a former NFL player. You're a former NFL coach. You're a former NFL player. You're a former NFL coach. You're a former NFL player. You're a former NFL coach. You're a former NFL player. You're a former NFL coach. You're a former NFL player. You're a former NFL coach. You're a former NFL player. You're a former NFL coach. You're a former NFL player. You're a former NFL coach. You're a former NFL player. You're a former NFL coach. You're a former NFL player. You're a former NFL coach. You're a former NFL player. You're a former NFL coach. You're a former NFL player. You're a former NFL coach. You're a former NFL player"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#model \"gpt-35-turbo\"  \n",
    "#You can see that gpt-35-turbo has been trained in QnA conversational style.\n",
    "answer=llm(\"Good morning, how are you?\")\n",
    "display (HTML(\"gpt-35-turbo: \" + answer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "gpt-35-turbo:  Use a for loop, not the built-in reverse() function.\n",
       "\n",
       "def reverse_string(string):\n",
       "    reversed_string = \"\"\n",
       "    for i in range(len(string)-1, -1, -1):\n",
       "        reversed_string += string[i]\n",
       "    return reversed_string\n",
       "\n",
       "print(reverse_string(\"hello\")) # \"olleh\"\n",
       "print(reverse_string(\"racecar\")) # \"racecar\"\n",
       "print(reverse_string(\"python\")) # \"nohtyp\"<|im_sep|>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "llm=init_llm()\n",
    "answer=llm(\"Create a Python function that takes a string argument and reverses it.\")\n",
    "display (Markdown(\"gpt-35-turbo: \" + answer))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prompts with LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "gpt-35-turbo:  To assess the risk tolerance of a new client, you can use a risk tolerance questionnaire. This questionnaire will ask the client a series of questions about their investment goals, investment experience, and risk preferences. Based on the answers provided, you can determine the client's risk tolerance and recommend an investment strategy that aligns with their risk tolerance."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain import PromptTemplate\n",
    "\n",
    "#create template for prompt\n",
    "\n",
    "template = \"\"\"You are a {profession} answering users questions. \n",
    "            More specifically, you are an expert in {expertise}. Answer in a clear and concise manner. Assume that the user is not a subject expert.\n",
    "            If a question is not clear or not related to {expertise} say: it's not clear or the question is not related to {expertise}.\n",
    "            \n",
    "            USER: {question}\n",
    "            ASSISTANT:\n",
    "            \n",
    "            <|im_end|>\n",
    "            \"\"\"\n",
    "#init LLM model\n",
    "llm = init_llm()\n",
    "prompt_template = PromptTemplate(template=template, input_variables=[\"profession\", \"expertise\", \"question\"])\n",
    "answer = llm(prompt_template.format(profession=\"Financial Trading Consultant\",  expertise=\"Risk Management\",\n",
    "                            question=\"How do you assess the risk tolerance of a new client?\"))\n",
    "display (Markdown(\"gpt-35-turbo: \" + answer))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "gpt-35-turbo:  It's not clear or the question is not related to Risk Management.<|im_end|>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#asking not related question\n",
    "prompt_template = PromptTemplate(template=template, input_variables=[\"profession\", \"expertise\", \"question\"])\n",
    "answer = llm(prompt_template.format(profession=\"Financial Trading Consultant\",  expertise=\"Risk Management\",\n",
    "                            question=\"What's the fastest car in the world?\"))\n",
    "display (Markdown(\"gpt-35-turbo: \" + answer))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ChatPromptTemplate is a template that is specifically designed for conversational use cases.\n",
    "\n",
    "It infers the variables from the template, so no need to pass them in as arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"You are a {profession} answering users questions. \n",
    "            More specifically, you are an expert in {expertise}. Answer in a clear and concise manner. Assume that the user is not a subject expert.\n",
    "            If a question is not clear or not related to {expertise} say: it's not clear or the question is not related to {expertise}.\n",
    "            \n",
    "            USER: {question}\n",
    "            ASSISTANT:\n",
    "            \n",
    "            <|im_end|>\n",
    "            \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_template(template)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "gpt-35-turbo:  It's not clear or the question is not related to Risk Management.<|im_end|>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#ChatPromptTemplate figures out templates variables automatically\n",
    "answer = llm(prompt_template.format(profession=\"Financial Trading Consultant\",  expertise=\"Risk Management\",\n",
    "                            question=\"What's the fastest car in the world?\"))\n",
    "display (Markdown(\"gpt-35-turbo: \" + answer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **OPTIONAL.** LLM output parsers with LangChain\n",
    "You can define the output schema and LangChain will parse the output for you.\n",
    "\n",
    "The main idea is to define parsing instructions as a code rather than doing it in textual form.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "customer_call = \"\"\"\n",
    "\n",
    "**Customer**: Hello, my name is John, and I'm a customer of Imaginal Bank.\n",
    "**Clerk**: Hello, John! My name is Sara, and I'm a customer service representative at Imaginal Bank. How can I assist you today?\n",
    "**Customer**: Hi, Sara. I'm interested in your bank's investment programs. \n",
    "              Can you tell me more about them, especially in terms of risk management?\n",
    "\n",
    "**Clerk**: Absolutely, John. We have a few key programs I can highlight.\n",
    "\n",
    "First, there's our 'Balanced Growth Fund'. It's a diversified mutual fund that invests in a mix of equities and bonds to provide both growth and income, reducing risk through diversification. \n",
    "\n",
    "We also have the 'Index Tracker ETF', which is designed to replicate the performance of a specific market index. By spreading investments across the entire index, it inherently reduces the risk associated with individual stocks.\n",
    "\n",
    "Additionally, for those with a lower risk tolerance, we have the 'Secure Income Bond Fund', which focuses on government and high-quality corporate bonds. \n",
    "\n",
    "Our financial advisors are always available to guide you in choosing the right program based on your financial goals and risk tolerance.\n",
    "\n",
    "**Customer**: I see. Could you elaborate on how the Balanced Growth Fund manages risk?\n",
    "\n",
    "**Clerk**: Sure. The Balanced Growth Fund mitigates risk by diversifying investments across a wide range of assets. If one investment performs poorly, it's likely to be offset by other investments that are performing well. Furthermore, our portfolio managers actively manage the fund, adjusting holdings based on changing market conditions to manage risk and enhance returns.\n",
    "\n",
    "**Customer**: Does the bank provide any tools to monitor my investments?\n",
    "\n",
    "**Clerk**: Yes, John. We offer an online platform called 'Imaginal Investor Dashboard'. It provides real-time tracking of your investments, balance updates, and market trends. You can also set up alerts to be notified about significant changes in your portfolio.\n",
    "\n",
    "**Customer**: That sounds quite comprehensive. How can I get started?\n",
    "\n",
    "**Clerk**: You can schedule an appointment with one of our financial advisors. They'll walk you through your options, help you understand your risk tolerance, and guide you in choosing the right investment program. Would you like me to arrange that for you?\n",
    "\n",
    "**Customer**: Yes, please. That would be helpful.\n",
    "\n",
    "**Clerk**: Fantastic, John! Let's get that set up for you...\"\"\"\n",
    "\n",
    "\n",
    "call_center_prompt_template = \"\"\"\n",
    "\n",
    "For the following text, extract the following information:\n",
    "agent politeness: How polite is the agent? use the following values to descibe agent politenes: very polite, polite, neutral, impolite, very impolite.\n",
    "agent knowledge: How knowledgeable is the agent? use the following values to descibe agent knowledge: very knowledgeable, knowledgeable, neutral, not knowledgeable, very not knowledgeable.\n",
    "customer issue resolution: How well did the agent resolve the issue? use the following values to descibe issue resolution: very well, well, neutral, not well, very not well.\n",
    "customer satisfaction: How satisfied is the customer? use the following values to descibe customer satisfaction: very satisfied, satisfied, neutral, dissatisfied, very dissatisfied.\n",
    "\n",
    "Format the output as a json object with the following keys: agent_politeness, agent_knowledge, customer_issue_resolution, customer_satisfaction.\n",
    "\n",
    "text: {text}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Output:\n",
      "\n",
      "{\n",
      "    \"agent_politeness\": \"very polite\",\n",
      "    \"agent_knowledge\": \"knowledgeable\",\n",
      "    \"customer_issue_resolution\": \"well\",\n",
      "    \"customer_satisfaction\": \"very satisfied\"\n",
      "}<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "prompt_template = ChatPromptTemplate.from_template(call_center_prompt_template)\n",
    "#print (prompt_template)\n",
    "llm=init_llm()\n",
    "#Note that the type of the response is a string, it looks like a json object, but it's string\n",
    "response = llm(prompt_template.format(text=customer_call))\n",
    "print (response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import ResponseSchema\n",
    "from langchain.output_parsers import StructuredOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"\\`\\`\\`json\" and \"\\`\\`\\`\":\n",
      "\n",
      "```json\n",
      "{\n",
      "\t\"agent_politeness\": string  // How polite is the agent?\n",
      "\t\"agent_knowledge\": string  // How knowledgeable is the agent?\n",
      "\t\"customer_issue_resolution\": string  // How well did the agent resolve the issue?\n",
      "\t\"customer_satisfaction\": string  // How satisfied is the customer?\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "\n",
    "agent_politeness_schema = ResponseSchema(name=\"agent_politeness\",description=\"How polite is the agent?\")\n",
    "aggent_knowledge_schema = ResponseSchema(name=\"agent_knowledge\",description=\"How knowledgeable is the agent?\")\n",
    "customer_issue_resolution_schema = ResponseSchema(name=\"customer_issue_resolution\",description=\"How well did the agent resolve the issue?\")\n",
    "customer_satisfaction_schema = ResponseSchema(name=\"customer_satisfaction\",description=\"How satisfied is the customer?\")\n",
    "customer_service_schemas = [agent_politeness_schema,aggent_knowledge_schema,customer_issue_resolution_schema,customer_satisfaction_schema]\n",
    "\n",
    "output_parser = StructuredOutputParser.from_response_schemas(customer_service_schemas)\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "print (format_instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "call_center_prompt_template_with_output_parser = \"\"\"\n",
    "\n",
    "For the following text, extract the following information:\n",
    "agent politeness: How polite is the agent? use the following values to descibe agent politenes: very polite, polite, neutral, impolite, very impolite.\n",
    "agent knowledge: How knowledgeable is the agent? use the following values to descibe agent knowledge: very knowledgeable, knowledgeable, neutral, not knowledgeable, very not knowledgeable.\n",
    "customer issue resolution: How well did the agent resolve the issue? use the following values to descibe issue resolution: very well, well, neutral, not well, very not well.\n",
    "customer satisfaction: How satisfied is the customer? use the following values to descibe customer satisfaction: very satisfied, satisfied, neutral, dissatisfied, very dissatisfied.\n",
    "\n",
    "Format the output as a json object with the following keys: agent_politeness, agent_knowledge, customer_issue_resolution, customer_satisfaction.\n",
    "\n",
    "text: {text}\n",
    "\n",
    "{format_instructions}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_template(template=call_center_prompt_template_with_output_parser)\n",
    "input = prompt.format(text=customer_call,format_instructions=format_instructions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: \n",
      "\n",
      "For the following text, extract the following information:\n",
      "agent politeness: How polite is the agent? use the following values to descibe agent politenes: very polite, polite, neutral, impolite, very impolite.\n",
      "agent knowledge: How knowledgeable is the agent? use the following values to descibe agent knowledge: very knowledgeable, knowledgeable, neutral, not knowledgeable, very not knowledgeable.\n",
      "customer issue resolution: How well did the agent resolve the issue? use the following values to descibe issue resolution: very well, well, neutral, not well, very not well.\n",
      "customer satisfaction: How satisfied is the customer? use the following values to descibe customer satisfaction: very satisfied, satisfied, neutral, dissatisfied, very dissatisfied.\n",
      "\n",
      "Format the output as a json object with the following keys: agent_politeness, agent_knowledge, customer_issue_resolution, customer_satisfaction.\n",
      "\n",
      "text: \n",
      "\n",
      "**Customer**: Hello, my name is John, and I'm a customer of Imaginal Bank.\n",
      "**Clerk**: Hello, John! My name is Sara, and I'm a customer service representative at Imaginal Bank. How can I assist you today?\n",
      "**Customer**: Hi, Sara. I'm interested in your bank's investment programs. \n",
      "              Can you tell me more about them, especially in terms of risk management?\n",
      "\n",
      "**Clerk**: Absolutely, John. We have a few key programs I can highlight.\n",
      "\n",
      "First, there's our 'Balanced Growth Fund'. It's a diversified mutual fund that invests in a mix of equities and bonds to provide both growth and income, reducing risk through diversification. \n",
      "\n",
      "We also have the 'Index Tracker ETF', which is designed to replicate the performance of a specific market index. By spreading investments across the entire index, it inherently reduces the risk associated with individual stocks.\n",
      "\n",
      "Additionally, for those with a lower risk tolerance, we have the 'Secure Income Bond Fund', which focuses on government and high-quality corporate bonds. \n",
      "\n",
      "Our financial advisors are always available to guide you in choosing the right program based on your financial goals and risk tolerance.\n",
      "\n",
      "**Customer**: I see. Could you elaborate on how the Balanced Growth Fund manages risk?\n",
      "\n",
      "**Clerk**: Sure. The Balanced Growth Fund mitigates risk by diversifying investments across a wide range of assets. If one investment performs poorly, it's likely to be offset by other investments that are performing well. Furthermore, our portfolio managers actively manage the fund, adjusting holdings based on changing market conditions to manage risk and enhance returns.\n",
      "\n",
      "**Customer**: Does the bank provide any tools to monitor my investments?\n",
      "\n",
      "**Clerk**: Yes, John. We offer an online platform called 'Imaginal Investor Dashboard'. It provides real-time tracking of your investments, balance updates, and market trends. You can also set up alerts to be notified about significant changes in your portfolio.\n",
      "\n",
      "**Customer**: That sounds quite comprehensive. How can I get started?\n",
      "\n",
      "**Clerk**: You can schedule an appointment with one of our financial advisors. They'll walk you through your options, help you understand your risk tolerance, and guide you in choosing the right investment program. Would you like me to arrange that for you?\n",
      "\n",
      "**Customer**: Yes, please. That would be helpful.\n",
      "\n",
      "**Clerk**: Fantastic, John! Let's get that set up for you...\n",
      "\n",
      "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"\\`\\`\\`json\" and \"\\`\\`\\`\":\n",
      "\n",
      "```json\n",
      "{\n",
      "\t\"agent_politeness\": string  // How polite is the agent?\n",
      "\t\"agent_knowledge\": string  // How knowledgeable is the agent?\n",
      "\t\"customer_issue_resolution\": string  // How well did the agent resolve the issue?\n",
      "\t\"customer_satisfaction\": string  // How satisfied is the customer?\n",
      "}\n",
      "```\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print (input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = llm(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'very polite'"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict = output_parser.parse(response)\n",
    "dict.get(\"agent_politeness\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prompts management best practices\n",
    "Reuse prompts as much as possible. This will help you to get more consistent results.\n",
    "\n",
    "Treat your prompts as a code, keep it in a version control system. This will help you to track changes and to revert them if needed.\n",
    "\n",
    "Use LangChain specific to use case prompt templates, e.g. ChatPromptTemplate for conversational flows.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  **One-shot, Few-shot learning**\n",
    "\n",
    "This technique could improve model performance by a lot. \n",
    "We can use the model to learn from a few examples and then use it to generate text. This is called few-shot learning. We can also use the model to learn from a single example and then use it to generate text. This is called one-shot learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "template_few_shot = \"\"\"You are a {profession} answering users questions. \n",
    "            More specifically, you are an expert in {expertise}. Answer in a clear and concise manner. Assume that a user is not a subject expert.\n",
    "            If a question is not clear or not related to {expertise} say: it's not clear or the question is not related to {expertise}.\n",
    "           \n",
    "            USER: How do you assess the risk tolerance of a new client?\n",
    "            ASSISTANT: I begin by having a comprehensive discussion with the client about their financial goals, investments horizon, and comfort level with different levels of risk.\n",
    "            \n",
    "            USER: Can you provide an example of a specific risk management strategy you'd recommended to a client in a volatile market situation?\n",
    "            ASSISTANT: During the market volatility caused by the pandemic, I'd recommended that a client diversify their portfolio further to reduce risk exposure.\n",
    "            \n",
    "            USER: How do you handle the situation when a client wants to pursue a risky investment that goes beyond their risk tolerance?\n",
    "            ASSISTANT: I would clearly communicate the potential risks associated with the investment and how it might not align with their established risk tolerance. \n",
    "            \n",
    "            USER: {question}\n",
    "            ASSISTANT:\n",
    "            \n",
    "            <|im_end|>\n",
    "            \"\"\"\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       " There are many tools available to assist in risk management, including portfolio management software, risk assessment tools, and financial modeling software. I use these tools to help clients understand the potential risks associated with different investments and to develop strategies to mitigate those risks."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt_few_shot = PromptTemplate(template=template_few_shot, input_variables=[\"profession\", \"expertise\", \"question\"])\n",
    "chain = LLMChain(llm=llm, prompt=prompt_few_shot)\n",
    "\n",
    "res=chain.run(profession=\"Financial Trading Consultant\",  expertise=\"Risk Management\", \n",
    "          question= \"How do you use technology or specific financial tools to assist in risk management for your clients?\")\n",
    "display (Markdown(res))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Large Language Models (LLMs) are stateless. This means that they don’t retain any information about the conversation history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       " It's not clear or the question is not related to Risk Management."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Since we don't save a history of the conversation, the model will fail to answer questions that require context.\n",
    "res = chain.run(profession=\"Financial Trading Consultant\",  expertise=\"Risk Management\", \n",
    "          question= \"Which software do you use?\")\n",
    "display (Markdown(res))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Retain conversation history** "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The Large Language Models (LLMs) are stateless. This means that they don’t retain any information about the conversation history.\n",
    "Each transaction is independent of the previous one. Chatbots keep in memory the conversation history and use it to generate the next response. This is why they are able to generate more coherent responses.\n",
    "\n",
    "Previously we saw that a model fails to answer the question that requires context. We can solve this problem by retaining the conversation history. We can do this by using the LangChain ConversationBufferMemory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"You are a {profession} answering users questions. \n",
    "            More specifically, you are an expert in {expertise}. Answer in a clear and concise manner. Assume that the user is not a subject expert.\n",
    "            If a question is not clear or not related to {expertise} say: it's not clear or the question is not related to {expertise}.\n",
    "            \n",
    "            USER: {question}\n",
    "            ASSISTANT:\n",
    "\n",
    "            <|im_end|>\n",
    "            \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "llm=init_llm()\n",
    "\n",
    "#ConversationBufferMemory is a memory that stores the conversation history\n",
    "memory = ConversationBufferMemory()\n",
    "#try to change the verbose to True, to see more details\n",
    "conversation = ConversationChain(llm=llm, memory=memory, verbose=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       " Hello! I'm happy to help. There are many ways that technology and financial tools can be used to assist in risk management. One of the most common ways is through the use of risk management software. This software can help identify potential risks and provide tools to mitigate those risks. Additionally, there are many financial tools that can be used to help manage risk, such as options, futures, and other derivatives. These tools can be used to hedge against potential losses and protect against market volatility. Finally, there are many data analysis tools that can be used to help identify trends and patterns in financial data, which can be used to make more informed decisions about risk management. Does that help?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = ChatPromptTemplate.from_template(template=template)\n",
    "\n",
    "response = conversation.run (input=prompt.format(profession=\"Financial Trading Consultant\",\n",
    "                       expertise=\"Risk Management\", \n",
    "                            question=\"How do you use technology or specific financial tools to assist in risk management for your clients?\"))\n",
    "\n",
    "display (Markdown(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "  There are many different risk management software options available, and the specific software used can vary depending on the needs of the client. Some popular options include RiskMetrics, MSCI RiskManager, and Algorithmics. These software packages can help identify potential risks, provide tools to mitigate those risks, and help monitor risk exposure over time. Does that help?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Now with the conversation history, the model can answer questions that require context.\n",
    "response = conversation.run(input=prompt.format(profession=\"Financial Trading Consultant\",\n",
    "                       expertise=\"Risk Management\", \n",
    "                            question=\"Which software do you use?\"))\n",
    "\n",
    "display (Markdown(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "human: Human: You are a Financial Trading Consultant answering users questions. \n",
      "            More specifically, you are an expert in Risk Management. Answer in a clear and concise manner. Assume that the user is not a subject expert.\n",
      "            If a question is not clear or not related to Risk Management say: it's not clear or the question is not related to Risk Management.\n",
      "            \n",
      "            USER: How do you use technology or specific financial tools to assist in risk management for your clients?\n",
      "            ASSISTANT:\n",
      "\n",
      "            <|im_end|>\n",
      "            \n",
      "ai:  Hello! I'm happy to help. There are many ways that technology and financial tools can be used to assist in risk management. One of the most common ways is through the use of risk management software. This software can help identify potential risks and provide tools to mitigate those risks. Additionally, there are many financial tools that can be used to help manage risk, such as options, futures, and other derivatives. These tools can be used to hedge against potential losses and protect against market volatility. Finally, there are many data analysis tools that can be used to help identify trends and patterns in financial data, which can be used to make more informed decisions about risk management. Does that help?<|im_end|>\n",
      "human: Human: You are a Financial Trading Consultant answering users questions. \n",
      "            More specifically, you are an expert in Risk Management. Answer in a clear and concise manner. Assume that the user is not a subject expert.\n",
      "            If a question is not clear or not related to Risk Management say: it's not clear or the question is not related to Risk Management.\n",
      "            \n",
      "            USER: Which software do you use?\n",
      "            ASSISTANT:\n",
      "\n",
      "            <|im_end|>\n",
      "            \n",
      "ai:   There are many different risk management software options available, and the specific software used can vary depending on the needs of the client. Some popular options include RiskMetrics, MSCI RiskManager, and Algorithmics. These software packages can help identify potential risks, provide tools to mitigate those risks, and monitor risk exposure over time. Does that help?<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "#you can print the conversation history\n",
    "history = conversation.memory.chat_memory.messages\n",
    "for msg in history:\n",
    "    print ( f\"{msg.type}: {msg.content}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['history', 'input'] output_parser=None partial_variables={} template='The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\\n\\nCurrent conversation:\\n{history}\\nHuman: {input}\\nAI:' template_format='f-string' validate_template=True\n"
     ]
    }
   ],
   "source": [
    "#pay attention that LangChain added to the prompt: ```Current conversation:{history}```\n",
    "print(conversation.prompt)\n",
    "print (memory.load_memory_variables({}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "  Sure! You've asked me two questions so far. The first was about how technology and financial tools can be used to assist in risk management for clients. The second was about which software is commonly used for risk management. Is there anything else I can help you with?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ans = conversation.run (input = prompt.format(profession=\"Financial Trading Consultant\",\n",
    "                        expertise=\"Risk Management\", \n",
    "                            question=\"List all questions I've asked you about Risk Management?\"))\n",
    "display (Markdown(ans))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### More conversation memory types.\n",
    "\n",
    "Keeping full conversation history could be expensive and we can hit the Azure API limits. LangChain provides different types of conversation memory that can be used to keep the conversation history and mitigate the limits issues.\n",
    "\n",
    "**1. ConversationBufferWindowMemory** - keeps the last N messages in the conversation history.\n",
    "\n",
    "**2. ConversationTokenBufferMemory** - keeps the last N tokens in the conversation history.\n",
    "\n",
    "**3. ConversationSummaryBufferMemory** - keeps summary of the conversation over time.\n",
    "\n",
    "Additional Memory Type includes:\n",
    "\n",
    "**4. Vector data memory** - stires the text in a vector DB and retrieves most semantically similar text.\n",
    "\n",
    "**5. Entity memories** - use LLM which rememebers details about specific entities. \n",
    "\n",
    "Note: You can use multiple memories at the same time. \n",
    "\n",
    "Finally you can store the conevrsation history in a conventional database like SQL or NoSQL. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': \"Human: What's your name\\nAI: My name is John\"}"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "\n",
    "memory = ConversationBufferWindowMemory(k=1)\n",
    "memory.save_context({\"input\": \"hi\"}, {\"output\": \"hello\"})\n",
    "memory.save_context({\"input\": \"What's your name\"}, {\"output\": \"My name is John\"})\n",
    "\n",
    "memory.load_memory_variables({})\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

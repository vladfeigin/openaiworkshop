{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval Augmeneted Generation in Action\n",
    "\n",
    "All steps of RAG pattern are implemented in this notebook. We will use the same dataset as in the previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import json\n",
    "import openai\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.search.documents import SearchClient\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.chat_models import AzureChatOpenAI\n",
    "from langchain.llms import AzureOpenAI\n",
    "from dotenv import load_dotenv\n",
    "from tenacity import retry, wait_random_exponential, stop_after_attempt\n",
    "from IPython.display import display, HTML, JSON, Markdown\n",
    "\n",
    "# Configure environment variables\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://openailabs303474.openai.azure.com/\n"
     ]
    }
   ],
   "source": [
    "service_endpoint = os.getenv(\"AZURE_SEARCH_SERVICE_ENDPOINT\")\n",
    "index_name = os.getenv(\"AZURE_SEARCH_INDEX_NAME\")\n",
    "key = os.getenv(\"AZURE_SEARCH_ADMIN_KEY\")\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "OPENAI_DEPLOYMENT_ENDPOINT = os.getenv(\"OPENAI_DEPLOYMENT_ENDPOINT\")\n",
    "OPENAI_DEPLOYMENT_NAME = os.getenv(\"OPENAI_DEPLOYMENT_NAME\")\n",
    "OPENAI_MODEL_NAME = os.getenv(\"OPENAI_MODEL_NAME\")\n",
    "OPENAI_DEPLOYMENT_VERSION = os.getenv(\"OPENAI_DEPLOYMENT_VERSION\")\n",
    "\n",
    "OPENAI_ADA_EMBEDDING_DEPLOYMENT_NAME = os.getenv(\n",
    "    \"OPENAI_ADA_EMBEDDING_DEPLOYMENT_NAME\")\n",
    "OPENAI_ADA_EMBEDDING_MODEL_NAME = os.getenv(\"OPENAI_ADA_EMBEDDING_MODEL_NAME\")\n",
    "\n",
    "# Configure OpenAI API\n",
    "openai.api_type = \"azure\"\n",
    "openai.api_version = OPENAI_DEPLOYMENT_VERSION\n",
    "openai.api_base = OPENAI_DEPLOYMENT_ENDPOINT\n",
    "openai.api_key = OPENAI_API_KEY\n",
    "# ---\n",
    "credential = AzureKeyCredential(key)\n",
    "\n",
    "print(OPENAI_DEPLOYMENT_ENDPOINT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_llm(model=OPENAI_MODEL_NAME,\n",
    "             deployment_name=OPENAI_DEPLOYMENT_NAME,\n",
    "             temperature=0,\n",
    "             max_tokens=500,\n",
    "             ):\n",
    "\n",
    "    llm = AzureOpenAI(deployment_name=deployment_name,\n",
    "                      model=model,\n",
    "                      temperature=temperature,\n",
    "                      max_tokens=max_tokens,\n",
    "                      model_kwargs={\"stop\": [\"<|im_end|>\"]}\n",
    "                      )\n",
    "    return llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Document Embeddings using OpenAI Ada 002\n",
    "\n",
    "@retry(wait=wait_random_exponential(min=1, max=20), stop=stop_after_attempt(6))\n",
    "# Function to generate embeddings for title and content fields, also used for query embeddings\n",
    "def generate_embeddings(page):\n",
    "    response = openai.Embedding.create(\n",
    "        input=page, engine=\"text-embedding-ada-002\")\n",
    "\n",
    "    embeddings = response['data'][0]['embedding']\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\" System:\n",
    "You are assistant helping the company technical support users with their questions about different product features. \n",
    "Answer ONLY with the facts listed in the sources below delimited by triple backticks.\n",
    "If there isn't enough information in the Sources, say you don't know and ask a user to provide more details. \n",
    "Do not generate answers that don't use the Sources below. \n",
    "If asking a clarifying question to the user would help, ask the question. \n",
    "\n",
    "Sources:\n",
    "```{sources}```\n",
    "User question is here below delimited by triple backticks\n",
    "User:\n",
    "```{question}``` \n",
    "\n",
    "You answer is here below:\n",
    "Answer:\n",
    "<|im_end|>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Init LLM model \n",
    "\n",
    "We use here Langchain ConversationChain and ConversationBufferMemory to keep the context of the conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "llm = init_llm()\n",
    "# ConversationBufferMemory is a memory that stores the conversation history\n",
    "memory = ConversationBufferMemory()\n",
    "# try to change the verbose to True, to see more details\n",
    "conversation = ConversationChain(llm=llm, memory=memory, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Init Azure Cognitive Search client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "search_client = SearchClient(\n",
    "\n",
    "    service_endpoint, index_name=\"sk-cogsrch-vector-index-2\", credential=credential)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Search method for searching in Azure Cognitive Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def search(question, top_k=3):\n",
    "    results = search_client.search(\n",
    "        search_text=None,\n",
    "        vector=generate_embeddings(question),\n",
    "        top_k=top_k,\n",
    "        vector_fields=\"contentVector\",\n",
    "        select=[\"title\", \"content\"],\n",
    "    )\n",
    "\n",
    "    result_pages = []\n",
    "    for result in results:\n",
    "        result_pages.append(result['content'])\n",
    "\n",
    "    sources = \"\\n\\n\".join([page for page in result_pages])\n",
    "    return sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calling OpenAI with the retrieved documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def ask_openai(sources, question):\n",
    "    prompt = ChatPromptTemplate.from_template(template=template)\n",
    "    response = conversation.run(input=prompt.format(sources=sources,\n",
    "                                                    question=question))\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Put all RAG phases together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       " Semantic Kernel is an open-source SDK that lets you easily combine AI services like OpenAI, Azure OpenAI, and Hugging Face with conventional programming languages like C# and Python. By doing so, you can create AI apps that combine the best of both worlds. During Kevin Scott's talk The era of the AI Copilot, he showed how Microsoft powers its Copilot system with a stack of AI models and plugins. At the center of this stack is an AI orchestration layer that allows us to combine AI models and plugins together to create brand new experiences for users. Semantic Kernel is at the center of the copilot stack."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "question = \"what's semnatic kernel\"\n",
    "#1. Retrieval. Call Azure Cognitive Search to retrieve relevant documents\n",
    "sources = search(question, top_k=3)  # retrieval\n",
    "#print(sources)\n",
    "# 2. Generation. Call OpenAI to generate a final answer\n",
    "response = ask_openai(sources, question)\n",
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "  Semantic Kernel plans on providing support to the following languages: C#, Python, Java. While the overall architecture of the kernel is consistent across all languages, we made sure the SDK for each language follows common paradigms and styles in each language to make it feel native and easy to use. Today, not all features are available in all languages. The following tables show which features are available in each language. The üîÑ symbol indicates that the feature is partially implemented, please see the associated note column for more details. The ‚ùå symbol indicates that the feature is not yet available in that language; if you would like to see a feature implemented in a language, please consider contributing to the project or opening an issue."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "question = \"which programming languages are supported by semantic kernel?\"\n",
    "#1. Retrieval. Call Azure Cognitive Search to retrieve relevant documents\n",
    "sources = search(question, top_k=3)\n",
    "#print(sources)\n",
    "# 2. Generation. Call OpenAI to generate a final answer\n",
    "response = ask_openai(sources, question)\n",
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector Search is multi-lingual\n",
    "\n",
    "In this example we ask a question in Spanish on the text, which is in English. \n",
    "\n",
    "The answer is in Spansih since the question is in Spanish so OpenAI generates the answer in the same language as the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "   El kernel orquesta el ASK del usuario expresado como un objetivo. El planificador lo descompone en pasos en funci√≥n de los recursos disponibles. Los plugins son recursos personalizables construidos a partir de LLM AI prompts y c√≥digo nativo."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "question = \"¬øQu√© son el planificador sem√°ntico del kernel y el kernel?\"\n",
    "sources = search(question, top_k=3)\n",
    "#print(sources)\n",
    "response = ask_openai(sources, question)\n",
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "    You can deploy Semantic Kernel to Azure as a web app service by using the standard methods available to deploy an ASP.net web app. Alternatively, you can follow the steps below to manually build and upload your customized version of the Semantic Kernel service to Azure. First, at the command line, go to the '/webapi' directory and enter the following command: PowerShell. This will create a directory which contains all the files needed for a deployment. Zip the contents of that directory and store the resulting zip file on cloud storage, e.g. Azure Blob Container. Put its URI in the \"Package Uri\" field in the web deployment page you access through the \"Deploy to Azure\" buttons or use its URI as the value for the PackageUri parameter of the deployment scripts found on this page. Your deployment will then use your customized deployment package. That package will be used to create a new Azure web app, which will be configured to run your customized version of the Semantic Kernel service."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "question = \"explain how to deploy Semantic Kernel to Azure as a web app service\"\n",
    "sources = search(question, top_k=3)\n",
    "#print(sources)\n",
    "response = ask_openai(sources, question)\n",
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

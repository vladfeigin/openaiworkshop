{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Load Environment variables from .env file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain.chat_models import AzureChatOpenAI\n",
    "import openai\n",
    "import tiktoken\n",
    "from IPython.display import display, HTML, JSON, Markdown\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "load_dotenv()\n",
    "# env variables that are used by LangChain\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv(\"OPENAI_API_KEY\")\n",
    "os.environ['OPENAI_API_TYPE'] = \"azure\"\n",
    "os.environ['OPENAI_API_VERSION'] = os.getenv(\"OPENAI_DEPLOYMENT_VERSION\")\n",
    "os.environ['OPENAI_API_BASE'] = os.getenv(\"OPENAI_DEPLOYMENT_ENDPOINT\")\n",
    "\n",
    "OPENAI_DEPLOYMENT_ENDPOINT = os.getenv(\"OPENAI_DEPLOYMENT_ENDPOINT\")\n",
    "OPENAI_DEPLOYMENT_NAME = os.getenv(\"OPENAI_DEPLOYMENT_NAME\")\n",
    "OPENAI_MODEL_NAME = os.getenv(\"OPENAI_MODEL_NAME\")\n",
    "\n",
    "OPENAI_ADA_EMBEDDING_DEPLOYMENT_NAME = os.getenv(\n",
    "    \"OPENAI_ADA_EMBEDDING_DEPLOYMENT_NAME\")\n",
    "OPENAI_ADA_EMBEDDING_MODEL_NAME = os.getenv(\"OPENAI_ADA_EMBEDDING_MODEL_NAME\")\n",
    "\n",
    "#gpt-4-turbo\n",
    "OPENAI_GPT_4_TURBO_MODEL_NAME=\"gpt-4-turbo\"\n",
    "OPENAI_GPT_4_TURBO_DEPLOYMENT_NAME=\"gpt-4-turbo\"\n",
    "\n",
    "# Configure OpenAI API\n",
    "openai.api_type = \"azure\"\n",
    "openai.api_version = os.getenv(\"OPENAI_DEPLOYMENT_VERSION\")\n",
    "openai.api_base = os.getenv(\"OPENAI_DEPLOYMENT_ENDPOINT\")\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Model initialization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def init_llm(model=OPENAI_GPT_4_TURBO_MODEL_NAME,\n",
    "             deployment_name=OPENAI_GPT_4_TURBO_DEPLOYMENT_NAME,\n",
    "             temperature=0,\n",
    "             max_tokens=400,\n",
    "             top_p=1,\n",
    "             ):\n",
    "\n",
    "    llm = AzureChatOpenAI(deployment_name=deployment_name,\n",
    "                      model=model,\n",
    "                      temperature=temperature,\n",
    "                      max_tokens=max_tokens,\n",
    "                      model_kwargs={\"stop\": [\"<|im_end|>\"],\n",
    "                                    \"top_p\":top_p}\n",
    "                      )\n",
    "    return llm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### **Prompt engineering techniques**\n",
    "Those techniques are not specific to summarization, but can be used for any task.\n",
    "\n",
    "1. Use delimeters to clearly separate the exact text the model should summarize.\n",
    "\n",
    "    Delimiters could be any kind of punctuation, that separates specific pieces of text. \n",
    "    Tripple quotes, Triple backtics, Triple dashes, Angle brackets, XML tags, etc.\n",
    "\n",
    "2. Ask model for structured output, e.g json, html, etc. If you ask for a json define the structure of the json, e.g. what fields should be in the json.\n",
    "\n",
    "3. Check assumptions required to do the task.\n",
    "\n",
    "4. Few-shot prompting. Provide examples of completing tasks and then ask model to perform the task.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Input text\n",
    "\n",
    "text = f\"\"\"\n",
    "**Customer**: Hello, my name is John, and I'm a customer of Imaginal Bank.\n",
    "**Clerk**: Hello, John! My name is Sara, and I'm a customer service representative at Imaginal Bank. How can I assist you today?\n",
    "**Customer**: Hi, Sara. I'm interested in your bank's investment programs. \n",
    "              Can you tell me more about them, especially in terms of risk management?\n",
    "\n",
    "**Clerk**: Absolutely, John. We have a few key programs I can highlight.\n",
    "\n",
    "First, there's our 'Balanced Growth Fund'. It's a diversified mutual fund that invests in a mix of equities and bonds to provide both growth and income, reducing risk through diversification. \n",
    "\n",
    "We also have the 'Index Tracker ETF', which is designed to replicate the performance of a specific market index. By spreading investments across the entire index, it inherently reduces the risk associated with individual stocks.\n",
    "\n",
    "Additionally, for those with a lower risk tolerance, we have the 'Secure Income Bond Fund', which focuses on government and high-quality corporate bonds. \n",
    "\n",
    "Our financial advisors are always available to guide you in choosing the right program based on your financial goals and risk tolerance.\n",
    "\n",
    "**Customer**: I see. Could you elaborate on how the Balanced Growth Fund manages risk?\n",
    "\n",
    "**Clerk**: Sure. The Balanced Growth Fund mitigates risk by diversifying investments across a wide range of assets. If one investment performs poorly, it's likely to be offset by other investments that are performing well. Furthermore, our portfolio managers actively manage the fund, adjusting holdings based on changing market conditions to manage risk and enhance returns.\n",
    "\n",
    "**Customer**: Does the bank provide any tools to monitor my investments?\n",
    "\n",
    "**Clerk**: Yes, John. We offer an online platform called 'Imaginal Investor Dashboard'. It provides real-time tracking of your investments, balance updates, and market trends. You can also set up alerts to be notified about significant changes in your portfolio.\n",
    "\n",
    "**Customer**: That sounds quite comprehensive. How can I get started?\n",
    "\n",
    "**Clerk**: You can schedule an appointment with one of our financial advisors. They'll walk you through your options, help you understand your risk tolerance, and guide you in choosing the right investment program. Would you like me to arrange that for you?\n",
    "\n",
    "**Customer**: Yes, please. That would be helpful.\n",
    "\n",
    "**Clerk**: Fantastic, John! Let's get that set up for you...\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING! top_p is not default parameter.\n",
      "                    top_p was transferred to model_kwargs.\n",
      "                    Please confirm that top_p is what you intended.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AzureChatOpenAI(cache=None, verbose=False, callbacks=None, callback_manager=None, tags=None, metadata=None, client=<class 'openai.api_resources.chat_completion.ChatCompletion'>, model_name='gpt-4-turbo', temperature=0.0, model_kwargs={'stop': ['<|im_end|>'], 'top_p': 1}, openai_api_key='1d301c0202bb4e22b2755131812e536d', openai_api_base='https://gpt35-instruct-demo.openai.azure.com/', openai_organization='', openai_proxy='', request_timeout=None, max_retries=6, streaming=False, n=1, max_tokens=400, tiktoken_model_name=None, deployment_name='gpt-4-turbo', openai_api_type='azure', openai_api_version='2023-05-15')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = init_llm()\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "ChatCompletion (gpt-4-turbo) :Good morning! I'm here to assist you. How can I help you with your trivia questions today?"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "messages = [{\"role\": \"system\", \"content\": \"You are a HELPFUL assistant answering users trivia questions. Answer in a clear and concise manner.\"},\n",
    "            {\"role\": \"user\", \"content\": \"Good morning, how are you today?\"}]\n",
    "\n",
    "\n",
    "answer = openai.ChatCompletion.create(engine=\"gpt-4-turbo\",\n",
    "\n",
    "                                      messages=messages,)\n",
    "display(HTML(\"ChatCompletion (gpt-4-turbo) :\" +\n",
    "        answer.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import LangChain schema classes for messages\n",
    "from langchain.schema import (\n",
    "    SystemMessage,\n",
    "    HumanMessage,\n",
    "    AIMessage\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "gpt-4-turbo: Good morning! I'm feeling absolutely fantastic, thank you for asking! How about you? How's your day going so far? ðŸ˜Š"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "messages = [\n",
    "    SystemMessage(\n",
    "        content=\"You're a nice and happy assistant. Answer users' questions happily.\")\n",
    "]\n",
    "messages.append(\n",
    "    HumanMessage(content=\"Good morning, how are you today?\"))\n",
    "\n",
    "answer = llm(messages)\n",
    "\n",
    "display(HTML(f\"gpt-4-turbo: {answer.content}\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "gpt-4-turbo: Summary: John, a customer at Imaginal Bank, inquires about investment programs and risk management. Sara, the clerk, explains various funds, risk mitigation strategies, and offers an online monitoring tool, proposing a meeting with a financial advisor."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(\n",
    "        content=f\"\"\"Summarize the text delimited by triple backticks into a summary of 20 words. \n",
    "        Text to summarize```{text}```\n",
    "        Output the summary in the form: Summary: <summary>\"\"\"),\n",
    "]\n",
    "\n",
    "llm = init_llm(\"gpt-4-turbo\", \"gpt-4-turbo\")\n",
    "answer = llm(messages)\n",
    "display(HTML(f\"gpt-4-turbo: {answer.content}\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AzureOpenAI(cache=None, verbose=False, callbacks=None, callback_manager=None, tags=None, metadata=None, client=<class 'openai.api_resources.completion.Completion'>, model_name='gpt-35-turbo-instruct', temperature=0.0, max_tokens=400, top_p=1.0, frequency_penalty=0, presence_penalty=0, n=1, best_of=1, model_kwargs={'stop': ['<|im_end|>']}, openai_api_key='1d301c0202bb4e22b2755131812e536d', openai_api_base='https://gpt35-instruct-demo.openai.azure.com/', openai_organization='', openai_proxy='', batch_size=20, request_timeout=None, logit_bias={}, max_retries=6, streaming=False, allowed_special=set(), disallowed_special='all', tiktoken_model_name=None, deployment_name='gpt-35-turbo-instruct', openai_api_type='azure', openai_api_version='2023-05-15')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check llm object content. Pay attention that openai.api_resources.completion.Completion\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Summary: John, a customer of Imaginal Bank, inquires about investment programs and their risk management, and Sara, the bank's clerk, explains the various funds available, the risk mitigation strategies, and the online tools for monitoring investments, leading to John requesting an appointment with a financial advisor."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check the specific conditions/assumptions in the input text\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(\n",
    "        content=f\"\"\" You will be provided with a text delimited by triple quotes. \n",
    "If it contains utterances of a Customer and a Clerk, summarize the text into single sentence.\n",
    "If the text does not contain utterances of a Customer and a Clerk, \n",
    "then just write 'No customer and clerk utterances found'.\n",
    "\\\"\\\"\\\" {text}  \\\"\\\"\\\"\n",
    "Output a summary in the form: Summary: <summary> \"\"\"),\n",
    "]\n",
    "\n",
    "\n",
    "llm = init_llm()\n",
    "sum = llm(messages)\n",
    "display(Markdown(sum.content))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Give the model time to think. Chain of Thoughts.\n",
    "\n",
    "Very useful technique is to give the model time to think by specifying the strict steps (sub-tasks) to complete the task and asking for output in a specific format. This is a very powerful technique.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Summary: John, a customer of Imaginal Bank, inquires about the bank's investment programs and their risk management, and Sara, the customer service representative, explains the various programs, risk mitigation strategies, and tools for monitoring investments, offering to set up an appointment with a financial advisor.\n",
       "Translated summary: John, um cliente do Imaginal Bank, pergunta sobre os programas de investimento do banco e sua gestÃ£o de riscos, e Sara, a representante de atendimento ao cliente, explica os vÃ¡rios programas, estratÃ©gias de mitigaÃ§Ã£o de riscos e ferramentas para monitoramento de investimentos, oferecendo-se para marcar uma consulta com um consultor financeiro.\n",
       "Customer questions:\n",
       "1. Can you tell me more about the bank's investment programs, especially in terms of risk management?\n",
       "2. Could you elaborate on how the Balanced Growth Fund manages risk?\n",
       "3. Does the bank provide any tools to monitor my investments?\n",
       "4. How can I get started?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "messages = [SystemMessage (content = f\"\"\" Your task is to perform the following actions:\n",
    "1 - Summarize the text delimited by triple backticks into single sentence.\n",
    "2 - Translate the summary into Portuguese.\n",
    "3 - List the Customer questions.\n",
    "\n",
    "Use the following output format:\n",
    "Summary: <summary>\n",
    "Translated summary: <portuguese summary>\n",
    "Customer questions: <enumerated customer questions>\n",
    "\n",
    "Text: ```{text}``` \n",
    "\"\"\") ]\n",
    "\n",
    "llm = init_llm()\n",
    "sum = llm(messages)\n",
    "display(Markdown(sum.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Let's do some math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Incorrect. \n",
       "\n",
       "When the student was 2 years old, the sister was twice the age, which means the sister was 2 * 2 = 4 years old at that time. The age difference between the student and the sister is 4 - 2 = 2 years. Now that the student is 40 years old, the sister would be 40 + 2 = 42 years old."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "messages = [SystemMessage(content =  f\"\"\" Determine if the student's solution delimited by the triple backticks is correct or not.\n",
    "Question: When I was 2 years old, my sister was twice my age. I'm now 40 years old. How old is my sister now? \n",
    "Student's answer: ```The sister is now 80 years old.``` \n",
    "If the student answer is correct, write 'Correct', otherwise write 'Incorrect'. Explain your answer.  \n",
    "<|im_end|>\n",
    "\"\"\")]\n",
    "\n",
    "llm = init_llm()\n",
    "sum = llm(messages)\n",
    "display(HTML(sum.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Ask model to do its own solution and then to compare both solutions and conclude which is correct.\n",
    " Define the task as a list of sub-tasks and ask the model to perform them in a specific order (splitting the task into sub-tasks technique). \n",
    " Then ask the model to compare its own solution with the solution provided by the model and conclude which is correct.\n",
    " Ask model to share it reasoning for the conclusion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Actual solution steps:\n",
       "1. When I was 2 years old, my sister was twice my age, which means she was 2 * 2 = 4 years old.\n",
       "2. The age difference between me and my sister is 4 - 2 = 2 years.\n",
       "3. If I am now 40 years old, my sister must be 40 + 2 = 42 years old.\n",
       "\n",
       "Student's solution: The sister is now 80 years old.\n",
       "\n",
       "Student's solution is correct: False"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Question = f\"\"\"When I was 2 years old my sister was twice of my age . I'm now 40 years old, how old is my sister now?\"\"\"\n",
    "Student_Solution = f\"\"\" The sister is now 80 years old.\"\"\"\n",
    "\n",
    "messages = [ SystemMessage (content =  f\"\"\" Determine if the Student's Solution for the Question is correct or not.\n",
    "To solve the problem do the following:\n",
    "1 - First, work out your OWN solution to the problem. Evaluate your final result to make sure it is correct and adheres to the question's conditions. \n",
    "    Reason about every step of your solution and make sure it is correct. \n",
    "2 - Second, compare your solution to the student's solution and evaluate if the student's solution is correct or not.\n",
    "Don't decide if the student's solution is correct until you have done the problem yourself.\n",
    "\n",
    "Student's Solution: ```{Student_Solution}```\n",
    "Question: ```{Question}```\n",
    "\n",
    "\n",
    "Use the following output format:\n",
    "Actual solution steps: <your own solution steps>\n",
    "\\nStudent's solution: <student's solution>\n",
    "\\nStudent's solution is correct: <true/false>\n",
    "\n",
    "\"\"\")]\n",
    "\n",
    "llm = init_llm()\n",
    "sum = llm(messages)\n",
    "display(Markdown(sum.content))  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Hallucinations and how to avoid them\n",
    "\n",
    "Hallucination is when the model generates text that is not supported by the input.\n",
    "To reduce hallucinations, use the techniques listed above, and strictly instruct the model to find the relevant information in the input text. \n",
    "If the information is not in the input, instruct the model to generate a specific output, e.g. \"I don't know the answer to this question\". "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Iterative Prompt Development\n",
    "Prompt engineering is an iterative process. You almost never get the prompt right the first time.\n",
    "You start with the idea, then you implement the prompt, get the experimental resuslts, do the Error Analysis and iterate again.\n",
    "\n",
    "**Try -> Analyze -> Clarify Instructions -> Try again**\n",
    "\n",
    "In more advanced phases refine prompts with a batch of examples."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "John, a customer of Imaginal Bank, inquires about the bank's investment programs and their risk management strategies. Sara, the customer service representative, explains that the bank offers various investment options such as the 'Balanced Growth Fund', 'Index Tracker ETF', and 'Secure Income Bond Fund', each with different levels of risk management. She also mentions the 'Imaginal Investor Dashboard' for monitoring investments and offers to schedule an appointment with a financial advisor to help John get started."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "llm = init_llm()\n",
    "prompt_prefix = [SystemMessage(content = f\"\"\" Summarize the text delimited by triple backticks into 2-3 sentences: ```{text}``` \"\"\")]\n",
    "text=\" \"\n",
    "with open(\"./data/bank-call-center-transcript.txt\", 'r') as file:\n",
    "        text = file.read()\n",
    "        \n",
    "sum = llm(prompt_prefix)\n",
    "display(Markdown(sum.content))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### More advanced prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The clerk at Imaginal Bank, Sara, successfully greeted the customer, John, in a polite and professional manner. She accurately understood John's inquiry about the bank's investment programs and their risk management. Sara provided clear and detailed information about various investment options, including the 'Balanced Growth Fund', 'Index Tracker ETF', and 'Secure Income Bond Fund', explaining how each manages risk and suits different risk tolerances.\n",
       "\n",
       "Sara did not ask additional questions for clarification, as John's inquiry was straightforward. She discussed the benefits of each investment program and how risk is managed, particularly elaborating on the risk management strategy of the Balanced Growth Fund when John requested more information.\n",
       "\n",
       "Sara informed John about the 'Imaginal Investor Dashboard', a tool for monitoring investments, and explained its features. She invited John to take further action by offering to arrange an appointment with a financial advisor to guide him through the investment process. Sara offered assistance for the next steps and ended the conversation on a positive note by setting up the appointment for John. Overall, the clerk accomplished all the tasks effectively."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "llm = init_llm()\n",
    "\n",
    "text=\" \"\n",
    "with open(\"./data/bank-call-center-transcript.txt\", 'r') as file:\n",
    "        text = file.read()\n",
    "        \n",
    "prompt_prefix = [SystemMessage ( content = f\"\"\" Prepare a summary for the text delimited by the triple backticks```{text}``` based on the points mentioned below. \n",
    "Please evaluate whether the clerk successfully accomplished the following tasks:\n",
    "\n",
    "Greeting the customer politely and professionally.\n",
    "Accurately understanding the customer's inquiry.\n",
    "Providing clear and detailed information in response.\n",
    "Asking questions as needed for clarification.\n",
    "Discussing both benefits and risks with the customer.\n",
    "Explaining the tools and resources available to the customer.\n",
    "Inviting the customer to take further action.\n",
    "Offering assistance for the next steps.\n",
    "Ending the conversation on a positive note.\n",
    "\"\"\")\n",
    "]\n",
    "\n",
    "sum = llm(prompt_prefix)\n",
    "display(Markdown(sum.content))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Output in tabular format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "  <tr>\n",
       "    <td style=\"color:green\">Did the clerk greet the customer in a polite and professional manner?</td>\n",
       "    <td style=\"color:green\">Yes, Sara greeted John politely and introduced herself as a customer service representative ready to assist.</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td style=\"color:red\">Did the clerk begin the conversation on a negative note?</td>\n",
       "    <td style=\"color:green\">No, the conversation started on a positive note with Sara willing to assist.</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td style=\"color:green\">Did the clerk understand the customer's inquiry?</td>\n",
       "    <td style=\"color:green\">Yes, Sara understood John's interest in investment programs and risk management.</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td style=\"color:green\">Did the clerk provide clear and detailed information?</td>\n",
       "    <td style=\"color:green\">Yes, Sara provided detailed information about various investment programs and the risk management strategies involved.</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td style=\"color:red\">Did the clerk ask questions to clarify the situation?</td>\n",
       "    <td style=\"color:red\">No, Sara did not ask further questions to clarify but provided information based on John's initial inquiry.</td>\n",
       "  </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "llm = init_llm()\n",
    "\n",
    "text=\" \"\n",
    "with open(\"./data/bank-call-center-transcript.txt\", 'r') as file:\n",
    "        text = file.read()\n",
    "        \n",
    "prompt_prefix = [ SystemMessage( content = f\"\"\"Prepare a summary for the text delimited by the triple backticks:```{text}``` based on the points mentioned below.\n",
    "Generate the output as a HTML table, with 2 columns: Customer and Clerk.\n",
    "\n",
    "Assign a color code to each item based on the clerk's performance - \n",
    "items that were successfully addressed should be marked in green, \n",
    "whereas items that were not met should be highlighted in red.\n",
    "\n",
    "Here are the evaluation criterias to consider:\n",
    "\n",
    "Did the clerk greet the customer in a polite and professional manner?\n",
    "Did the clerk begin the conversation on a negative note?\n",
    "Did the clerk understand the customer's inquiry?\n",
    "Did the clerk provide clear and detailed information?\n",
    "Did the clerk ask questions to clarify the situation?\n",
    "\"\"\" )]\n",
    "\n",
    "res = llm(prompt_prefix)\n",
    "                     \n",
    "display(HTML(res.content))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Output as JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vladfeigin/myprojects/openai/workshops/dataai/openaiworkshop-new/.venv/lib/python3.10/site-packages/IPython/core/display.py:618: UserWarning: JSON expects JSONable dict or list, not JSON strings\n",
      "  warnings.warn(\"JSON expects JSONable dict or list, not JSON strings\")\n"
     ]
    },
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting value: line 1 column 1 (char 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m/Users/vladfeigin/myprojects/openai/workshops/dataai/openaiworkshop-new/openaiworkshop/Lab #2 - Create prompts for summarization, json, tabular answer/lab #gpt4-turbo.ipynb Cell 30\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/vladfeigin/myprojects/openai/workshops/dataai/openaiworkshop-new/openaiworkshop/Lab%20%232%20-%20Create%20prompts%20for%20summarization%2C%20json%2C%20tabular%20answer/lab%20%23gpt4-turbo.ipynb#X35sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m prompt_prefix \u001b[39m=\u001b[39m [ SystemMessage ( content \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\"\"\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m Evaluate the clerk performance from the text delimited by the triple backticks: ```\u001b[39m\u001b[39m{\u001b[39;00mtext\u001b[39m}\u001b[39;00m\u001b[39m``` based on the points mentioned below.\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/vladfeigin/myprojects/openai/workshops/dataai/openaiworkshop-new/openaiworkshop/Lab%20%232%20-%20Create%20prompts%20for%20summarization%2C%20json%2C%20tabular%20answer/lab%20%23gpt4-turbo.ipynb#X35sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/vladfeigin/myprojects/openai/workshops/dataai/openaiworkshop-new/openaiworkshop/Lab%20%232%20-%20Create%20prompts%20for%20summarization%2C%20json%2C%20tabular%20answer/lab%20%23gpt4-turbo.ipynb#X35sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mDid the clerk greet the customer in a polite and professional manner?\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/vladfeigin/myprojects/openai/workshops/dataai/openaiworkshop-new/openaiworkshop/Lab%20%232%20-%20Create%20prompts%20for%20summarization%2C%20json%2C%20tabular%20answer/lab%20%23gpt4-turbo.ipynb#X35sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/vladfeigin/myprojects/openai/workshops/dataai/openaiworkshop-new/openaiworkshop/Lab%20%232%20-%20Create%20prompts%20for%20summarization%2C%20json%2C%20tabular%20answer/lab%20%23gpt4-turbo.ipynb#X35sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m \u001b[39m\"\"\"\u001b[39m)]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/vladfeigin/myprojects/openai/workshops/dataai/openaiworkshop-new/openaiworkshop/Lab%20%232%20-%20Create%20prompts%20for%20summarization%2C%20json%2C%20tabular%20answer/lab%20%23gpt4-turbo.ipynb#X35sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m res \u001b[39m=\u001b[39m llm( prompt_prefix )\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/vladfeigin/myprojects/openai/workshops/dataai/openaiworkshop-new/openaiworkshop/Lab%20%232%20-%20Create%20prompts%20for%20summarization%2C%20json%2C%20tabular%20answer/lab%20%23gpt4-turbo.ipynb#X35sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m display(JSON((res\u001b[39m.\u001b[39;49mcontent)))\n",
      "File \u001b[0;32m~/myprojects/openai/workshops/dataai/openaiworkshop-new/.venv/lib/python3.10/site-packages/IPython/core/display.py:601\u001b[0m, in \u001b[0;36mJSON.__init__\u001b[0;34m(self, data, url, filename, expanded, metadata, root, **kwargs)\u001b[0m\n\u001b[1;32m    599\u001b[0m \u001b[39mif\u001b[39;00m kwargs:\n\u001b[1;32m    600\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmetadata\u001b[39m.\u001b[39mupdate(kwargs)\n\u001b[0;32m--> 601\u001b[0m \u001b[39msuper\u001b[39;49m(JSON, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(data\u001b[39m=\u001b[39;49mdata, url\u001b[39m=\u001b[39;49murl, filename\u001b[39m=\u001b[39;49mfilename)\n",
      "File \u001b[0;32m~/myprojects/openai/workshops/dataai/openaiworkshop-new/.venv/lib/python3.10/site-packages/IPython/core/display.py:320\u001b[0m, in \u001b[0;36mDisplayObject.__init__\u001b[0;34m(self, data, url, filename, metadata)\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfilename \u001b[39m=\u001b[39m filename\n\u001b[1;32m    317\u001b[0m \u001b[39m# because of @data.setter methods in\u001b[39;00m\n\u001b[1;32m    318\u001b[0m \u001b[39m# subclasses ensure url and filename are set\u001b[39;00m\n\u001b[1;32m    319\u001b[0m \u001b[39m# before assigning to self.data\u001b[39;00m\n\u001b[0;32m--> 320\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdata \u001b[39m=\u001b[39m data\n\u001b[1;32m    322\u001b[0m \u001b[39mif\u001b[39;00m metadata \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    323\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmetadata \u001b[39m=\u001b[39m metadata\n",
      "File \u001b[0;32m~/myprojects/openai/workshops/dataai/openaiworkshop-new/.venv/lib/python3.10/site-packages/IPython/core/display.py:619\u001b[0m, in \u001b[0;36mJSON.data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    617\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfilename \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39murl \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    618\u001b[0m         warnings\u001b[39m.\u001b[39mwarn(\u001b[39m\"\u001b[39m\u001b[39mJSON expects JSONable dict or list, not JSON strings\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 619\u001b[0m     data \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39;49mloads(data)\n\u001b[1;32m    620\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_data \u001b[39m=\u001b[39m data\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.10/3.10.13_1/Frameworks/Python.framework/Versions/3.10/lib/python3.10/json/__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    341\u001b[0m     s \u001b[39m=\u001b[39m s\u001b[39m.\u001b[39mdecode(detect_encoding(s), \u001b[39m'\u001b[39m\u001b[39msurrogatepass\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    343\u001b[0m \u001b[39mif\u001b[39;00m (\u001b[39mcls\u001b[39m \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m object_hook \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m\n\u001b[1;32m    344\u001b[0m         parse_int \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m parse_float \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m\n\u001b[1;32m    345\u001b[0m         parse_constant \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m object_pairs_hook \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m kw):\n\u001b[0;32m--> 346\u001b[0m     \u001b[39mreturn\u001b[39;00m _default_decoder\u001b[39m.\u001b[39;49mdecode(s)\n\u001b[1;32m    347\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mcls\u001b[39m \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    348\u001b[0m     \u001b[39mcls\u001b[39m \u001b[39m=\u001b[39m JSONDecoder\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.10/3.10.13_1/Frameworks/Python.framework/Versions/3.10/lib/python3.10/json/decoder.py:337\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecode\u001b[39m(\u001b[39mself\u001b[39m, s, _w\u001b[39m=\u001b[39mWHITESPACE\u001b[39m.\u001b[39mmatch):\n\u001b[1;32m    333\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[1;32m    334\u001b[0m \u001b[39m    containing a JSON document).\u001b[39;00m\n\u001b[1;32m    335\u001b[0m \n\u001b[1;32m    336\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 337\u001b[0m     obj, end \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mraw_decode(s, idx\u001b[39m=\u001b[39;49m_w(s, \u001b[39m0\u001b[39;49m)\u001b[39m.\u001b[39;49mend())\n\u001b[1;32m    338\u001b[0m     end \u001b[39m=\u001b[39m _w(s, end)\u001b[39m.\u001b[39mend()\n\u001b[1;32m    339\u001b[0m     \u001b[39mif\u001b[39;00m end \u001b[39m!=\u001b[39m \u001b[39mlen\u001b[39m(s):\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.10/3.10.13_1/Frameworks/Python.framework/Versions/3.10/lib/python3.10/json/decoder.py:355\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    353\u001b[0m     obj, end \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscan_once(s, idx)\n\u001b[1;32m    354\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m--> 355\u001b[0m     \u001b[39mraise\u001b[39;00m JSONDecodeError(\u001b[39m\"\u001b[39m\u001b[39mExpecting value\u001b[39m\u001b[39m\"\u001b[39m, s, err\u001b[39m.\u001b[39mvalue) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    356\u001b[0m \u001b[39mreturn\u001b[39;00m obj, end\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)"
     ]
    }
   ],
   "source": [
    "llm = init_llm()\n",
    "\n",
    "text=\" \"\n",
    "with open(\"./data/bank-call-center-transcript.txt\", 'r') as file:\n",
    "        text = file.read()\n",
    "        \n",
    "prompt_prefix = [ SystemMessage ( content = f\"\"\"\" Evaluate the clerk performance from the text delimited by the triple backticks: ```{text}``` based on the points mentioned below.\n",
    "\n",
    "Did the clerk greet the customer in a polite and professional manner?\n",
    "Did the clerk comprehend the customer's inquiry accurately?\n",
    "Did the clerk provide comprehensive and clear information?\n",
    "Did the clerk ask relevant questions to clarify the customer's situation?\n",
    "Did the clerk explain the benefits and potential risks to the customer?\n",
    "Did the clerk detail the tools and resources available to the customer?\n",
    "Did the clerk encourage the customer to take further action?\n",
    "Did the clerk offer assistance with proceeding to the next steps?\n",
    "Did the clerk end the interaction on a positive and upbeat note?\"\n",
    "\n",
    "The output should be presented in JSON format, adhering to the following key-value pairs:\n",
    "\n",
    "\"Greet the Customer Politely and Professionally\": \"Yes/No\",\n",
    "\"Understand the Customer's Inquiry\": \"Yes/No\",\n",
    "\"Provide Clear and Detailed Information\": \"Yes/No\",\n",
    "\"Ask Clarifying Questions\": \"Yes/No\",\n",
    "\"Discuss the Benefits and Risks\": \"Yes/No\",\n",
    "\"Explain Available Tools and Resources\": \"Yes/No\",\n",
    "\"Invite Further Action\": \"Yes/No\",\n",
    "\"Offer to Assist with Next Steps\": \"Yes/No\",\n",
    "\"End on a Positive Note\": \"Yes/No\"\n",
    "\n",
    "\"\"\")]\n",
    "\n",
    "res = llm( prompt_prefix )\n",
    "display(JSON(json.loads(res.content)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarize large documents\n",
    "To summarize larger documents, we can split the document into smaller chunks and summarize each chunk separately. We can then combine the summaries of each chunk to get the final summary.\n",
    "LangChain has a built-in chain for doing that. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " \"./data/Large_language_model.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens in the document: 13420\n"
     ]
    }
   ],
   "source": [
    "from langchain import OpenAI\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "llm = init_llm()\n",
    "\n",
    "large_pdf_path = \"./data/Large_language_model.pdf\"\n",
    "loader = PyPDFLoader(large_pdf_path)\n",
    "\n",
    "# output type is List[Document]\n",
    "pages = loader.load_and_split()\n",
    "\n",
    "# count tokens in the document\n",
    "total_tokens = 0\n",
    "encoder = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "for page in pages:\n",
    "    total_tokens += len(encoder.encode(page.page_content))\n",
    "\n",
    "# The document has more 13000 tokens. This is too many for the LLM to process in one go.\n",
    "# This is whay we split the document into smaller chunks.\n",
    "print(f\"Total tokens in the document: {total_tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.summarize import load_summarize_chain\n",
    "\n",
    "# create summarization chain\n",
    "# map_reduce summarizes each document on it's own in a \"map\" step and then \"reduce\" the summaries into a final summary\n",
    "summary_chain = load_summarize_chain( \n",
    "    llm=llm, chain_type='map_reduce', verbose=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### There are following chain_types: stuff, map_reduce, refine, map-rerank\n",
    "See here for the details: https://docs.langchain.com/docs/components/chains/index_related_chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       " This article discusses various large language models, their capabilities, and their potential applications. It also covers the emergence of new models and techniques, such as instruction fine-tuning and prompting, and the ethical considerations surrounding these models. The article also mentions the upcoming release of several new models and their potential uses in different industries."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Many calls in loop causing RateLimitError: Too Many Requests. In this sum = summary_chain.run(pages[0:10]) \n",
    "sum = summary_chain.run(pages)\n",
    "\n",
    "display(Markdown(sum))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO - Upgrade LangChain version to the latest\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.chains.mapreduce import MapReduceChain\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.chains import ReduceDocumentsChain, MapReduceDocumentsChain, StuffDocumentsChain\n",
    "\n",
    "llm = init_llm()\n",
    "\n",
    "# Map\n",
    "map_template = \"\"\"The following is a set of documents\n",
    "{docs}\n",
    "Based on this list of docs, please identify the main themes \n",
    "Helpful Answer:\n",
    "<|im_end|>\"\"\"\n",
    "\n",
    "map_prompt = PromptTemplate.from_template(map_template)\n",
    "map_chain = LLMChain(llm=llm, prompt=map_prompt)\n",
    "\n",
    "# Reduce\n",
    "reduce_template = \"\"\"The following is set of summaries:\n",
    "{doc_summaries}\n",
    "Take these and distill it into a final, consolidated summary of the main themes. \n",
    "Summary:\n",
    "\n",
    "<|im_end|>\n",
    "\"\"\"\n",
    "reduce_prompt = PromptTemplate.from_template(reduce_template)\n",
    "reduce_chain = LLMChain(llm=llm, prompt=reduce_prompt)\n",
    "\n",
    "# Takes a list of documents, combines them into a single string, and passes this to an LLMChain\n",
    "combine_documents_chain = StuffDocumentsChain(\n",
    "    llm_chain=reduce_chain, document_variable_name=\"doc_summaries\"\n",
    ")\n",
    "\n",
    "# Combines and iteravely reduces the mapped documents\n",
    "reduce_documents_chain = ReduceDocumentsChain(\n",
    "    # This is final chain that is called.\n",
    "    combine_documents_chain=combine_documents_chain,\n",
    "    # If documents exceed context for `StuffDocumentsChain`\n",
    "    collapse_documents_chain=combine_documents_chain,\n",
    "    # The maximum number of tokens to group documents into.\n",
    "    token_max=2000,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining documents by mapping a chain over them, then combining results\n",
    "map_reduce_chain = MapReduceDocumentsChain(\n",
    "    # Map chain\n",
    "    llm_chain=map_chain,\n",
    "    # Reduce chain\n",
    "    reduce_documents_chain=reduce_documents_chain,\n",
    "    # The variable name in the llm_chain to put the documents in\n",
    "    document_variable_name=\"docs\",\n",
    "    # Return the results of the map steps in the output\n",
    "    return_intermediate_steps=False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Large language models have become powerful tools in natural language processing, with their capabilities and performance constantly improving. However, concerns about training costs and potential biases remain. Pre-training and fine-tuning techniques are used to optimize these models for specific tasks. Open-source models and collaborations have made these models more accessible, leading to advancements in dialogue and conversational AI. Ethical considerations and responsible AI are also important factors in their development and use. The release of new models, partnerships, and advancements in training data and computing are driving the future of AI and language models, with potential for impact in various industries.'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "map_reduce_chain.run(pages)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Load Environment variables from .env file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import AzureOpenAI\n",
    "import openai\n",
    "import tiktoken \n",
    "#to avoid error:Could not automatically map gpt-35-turbo to a tokeniser...\n",
    "tiktoken.model.MODEL_TO_ENCODING[\"gpt-35-turbo\"] = \"cl100k_base\"\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from IPython.display import display, HTML, JSON, Markdown\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\") \n",
    "OPENAI_DEPLOYMENT_ENDPOINT = os.getenv(\"OPENAI_DEPLOYMENT_ENDPOINT\")\n",
    "OPENAI_DEPLOYMENT_NAME = os.getenv(\"OPENAI_DEPLOYMENT_NAME\")\n",
    "OPENAI_MODEL_NAME = os.getenv(\"OPENAI_MODEL_NAME\")\n",
    "OPENAI_DEPLOYMENT_VERSION = os.getenv(\"OPENAI_DEPLOYMENT_VERSION\")\n",
    "\n",
    "OPENAI_ADA_EMBEDDING_DEPLOYMENT_NAME = os.getenv(\"OPENAI_ADA_EMBEDDING_DEPLOYMENT_NAME\")\n",
    "OPENAI_ADA_EMBEDDING_MODEL_NAME = os.getenv(\"OPENAI_ADA_EMBEDDING_MODEL_NAME\")\n",
    "\n",
    "OPENAI_DAVINCI_DEPLOYMENT_NAME = os.getenv(\"OPENAI_DAVINCI_DEPLOYMENT_NAME\")\n",
    "OPENAI_DAVINCI_MODEL_NAME = os.getenv(\"OPENAI_DAVINCI_MODEL_NAME\")\n",
    "\n",
    "# Configure OpenAI API\n",
    "openai.api_type = \"azure\"\n",
    "openai.api_version = OPENAI_DEPLOYMENT_VERSION\n",
    "openai.api_base = OPENAI_DEPLOYMENT_ENDPOINT\n",
    "openai.api_key = OPENAI_API_KEY"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Model initialization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def init_llm(model=OPENAI_MODEL_NAME,\n",
    "             deployment_name=OPENAI_DEPLOYMENT_NAME, \n",
    "             temperature=0,\n",
    "             max_tokens=400,\n",
    "             top_p=1,\n",
    "             ):\n",
    "    \n",
    "    llm = AzureOpenAI(deployment_name=deployment_name,  \n",
    "                  model=model,\n",
    "                  temperature=temperature,\n",
    "                  max_tokens=max_tokens,\n",
    "                  top_p=top_p,\n",
    "                  model_kwargs={\"stop\": [\"<|im_end|>\"]}\n",
    "    ) \n",
    "    return llm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### **Prompt engineering techniques**\n",
    "Those techniques are not specific to summarization, but can be used for any task.\n",
    "\n",
    "1. Use delimeters to clearly separate the exact text the model should summarize.\n",
    "\n",
    "    Delimiters could be any kind of punctuation, that separates specific pieces of text. \n",
    "    Tripple quotes, Triple backtics, Triple dashes, Angle brackets, XML tags, etc.\n",
    "\n",
    "2. Ask model for structured output, e.g json, html, etc. If you ask for a json define the structure of the json, e.g. what fields should be in the json.\n",
    "\n",
    "3. Check assumptions required to do the task.\n",
    "\n",
    "4. Few-shot prompting. Provide examples of completing tasks and then ask model to perform the task.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Summary: Imaginal Bank offers investment programs such as Balanced Growth Fund, Index Tracker ETF, and Secure Income Bond Fund. The bank mitigates risk by diversifying investments and actively managing the fund. The Imaginal Investor Dashboard provides real-time tracking of investments, balance updates, and market trends. Customers can schedule an appointment with a financial advisor to choose the right investment program."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Using delimiters to clearly separate input text.\n",
    "\n",
    "text = f\"\"\"\n",
    "**Customer**: Hello, my name is John, and I'm a customer of Imaginal Bank.\n",
    "**Clerk**: Hello, John! My name is Sara, and I'm a customer service representative at Imaginal Bank. How can I assist you today?\n",
    "**Customer**: Hi, Sara. I'm interested in your bank's investment programs. \n",
    "              Can you tell me more about them, especially in terms of risk management?\n",
    "\n",
    "**Clerk**: Absolutely, John. We have a few key programs I can highlight.\n",
    "\n",
    "First, there's our 'Balanced Growth Fund'. It's a diversified mutual fund that invests in a mix of equities and bonds to provide both growth and income, reducing risk through diversification. \n",
    "\n",
    "We also have the 'Index Tracker ETF', which is designed to replicate the performance of a specific market index. By spreading investments across the entire index, it inherently reduces the risk associated with individual stocks.\n",
    "\n",
    "Additionally, for those with a lower risk tolerance, we have the 'Secure Income Bond Fund', which focuses on government and high-quality corporate bonds. \n",
    "\n",
    "Our financial advisors are always available to guide you in choosing the right program based on your financial goals and risk tolerance.\n",
    "\n",
    "**Customer**: I see. Could you elaborate on how the Balanced Growth Fund manages risk?\n",
    "\n",
    "**Clerk**: Sure. The Balanced Growth Fund mitigates risk by diversifying investments across a wide range of assets. If one investment performs poorly, it's likely to be offset by other investments that are performing well. Furthermore, our portfolio managers actively manage the fund, adjusting holdings based on changing market conditions to manage risk and enhance returns.\n",
    "\n",
    "**Customer**: Does the bank provide any tools to monitor my investments?\n",
    "\n",
    "**Clerk**: Yes, John. We offer an online platform called 'Imaginal Investor Dashboard'. It provides real-time tracking of your investments, balance updates, and market trends. You can also set up alerts to be notified about significant changes in your portfolio.\n",
    "\n",
    "**Customer**: That sounds quite comprehensive. How can I get started?\n",
    "\n",
    "**Clerk**: You can schedule an appointment with one of our financial advisors. They'll walk you through your options, help you understand your risk tolerance, and guide you in choosing the right investment program. Would you like me to arrange that for you?\n",
    "\n",
    "**Customer**: Yes, please. That would be helpful.\n",
    "\n",
    "**Clerk**: Fantastic, John! Let's get that set up for you...\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "prompt = f\"\"\" Summarize the text delimited by triple backticks into a summary of 20 words. \n",
    "```{text}```\n",
    "\n",
    "Output the summary in the form: Summary: <summary> <|im_end|>\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "llm=init_llm()\n",
    "sum = llm(prompt)\n",
    "display(Markdown(sum))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Summary: John is interested in Imaginal Bank's investment programs and asks Sara about them. Sara explains the Balanced Growth Fund, Index Tracker ETF, and Secure Income Bond Fund. John asks about how the Balanced Growth Fund manages risk and if the bank provides tools to monitor investments. Sara explains how the Balanced Growth Fund mitigates risk and tells John about the Imaginal Investor Dashboard. John schedules an appointment with a financial advisor."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check the specific conditions/assumptions in the text\n",
    "\n",
    "prompt = f\"\"\" You will be provided with a text delimited by triple quotes. \n",
    "If it's contains utterances of a Customer and a Clerk, summarize the text into single sentence.\n",
    "If the text does not contain utterances of a Customer and a Clerk, \n",
    "then just write 'No customer and clerk utterances found'.\n",
    "\\\"\\\"\\\" {text}  \\\"\\\"\\\"\n",
    "\n",
    "Output the summary in the form: Summary: <summary> <|im_end|>\n",
    "\"\"\"\n",
    "llm=init_llm()\n",
    "sum = llm(prompt)\n",
    "display(Markdown(sum))\n",
    " "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Give the model time to think\n",
    "\n",
    "Very useful technique is to give the model time to think by specifying the strict steps (sub-tasks) to complete the task and asking for output in a specific format. This is a very powerful technique.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Summary: The customer is interested in Imaginal Bank's investment programs and asks about risk management. The clerk explains the Balanced Growth Fund, Index Tracker ETF, and Secure Income Bond Fund, and how they manage risk. The clerk also mentions the Imaginal Investor Dashboard, which provides real-time tracking of investments and market trends. The customer schedules an appointment with a financial advisor to get started.\n",
       "\n",
       "Translated summary: O cliente est√° interessado nos programas de investimento do Imaginal Bank e pergunta sobre gerenciamento de risco. O atendente explica o Balanced Growth Fund, o Index Tracker ETF e o Secure Income Bond Fund, e como eles gerenciam o risco. O atendente tamb√©m menciona o Imaginal Investor Dashboard, que fornece rastreamento em tempo real de investimentos e tend√™ncias de mercado. O cliente agenda uma consulta com um consultor financeiro para come√ßar.\n",
       "\n",
       "Customer questions:\n",
       "1. Can you tell me more about your bank's investment programs?\n",
       "2. How do your investment programs manage risk?\n",
       "3. Does the bank provide any tools to monitor my investments?\n",
       "4. How can I get started with investing?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "text = f\"\"\"\n",
    "**Customer**: Hello, my name is John, and I'm a customer of Imaginal Bank.\n",
    "**Clerk**: Hello, John! My name is Sara, and I'm a customer service representative at Imaginal Bank. How can I assist you today?\n",
    "**Customer**: Hi, Sara. I'm interested in your bank's investment programs. \n",
    "              Can you tell me more about them, especially in terms of risk management?\n",
    "\n",
    "**Clerk**: Absolutely, John. We have a few key programs I can highlight.\n",
    "\n",
    "First, there's our 'Balanced Growth Fund'. It's a diversified mutual fund that invests in a mix of equities and bonds to provide both growth and income, reducing risk through diversification. \n",
    "\n",
    "We also have the 'Index Tracker ETF', which is designed to replicate the performance of a specific market index. By spreading investments across the entire index, it inherently reduces the risk associated with individual stocks.\n",
    "\n",
    "Additionally, for those with a lower risk tolerance, we have the 'Secure Income Bond Fund', which focuses on government and high-quality corporate bonds. \n",
    "\n",
    "Our financial advisors are always available to guide you in choosing the right program based on your financial goals and risk tolerance.\n",
    "\n",
    "**Customer**: I see. Could you elaborate on how the Balanced Growth Fund manages risk?\n",
    "\n",
    "**Clerk**: Sure. The Balanced Growth Fund mitigates risk by diversifying investments across a wide range of assets. If one investment performs poorly, it's likely to be offset by other investments that are performing well. Furthermore, our portfolio managers actively manage the fund, adjusting holdings based on changing market conditions to manage risk and enhance returns.\n",
    "\n",
    "**Customer**: Does the bank provide any tools to monitor my investments?\n",
    "\n",
    "**Clerk**: Yes, John. We offer an online platform called 'Imaginal Investor Dashboard'. It provides real-time tracking of your investments, balance updates, and market trends. You can also set up alerts to be notified about significant changes in your portfolio.\n",
    "\n",
    "**Customer**: That sounds quite comprehensive. How can I get started?\n",
    "\n",
    "**Clerk**: You can schedule an appointment with one of our financial advisors. They'll walk you through your options, help you understand your risk tolerance, and guide you in choosing the right investment program. Would you like me to arrange that for you?\n",
    "\n",
    "**Customer**: Yes, please. That would be helpful.\n",
    "\n",
    "**Clerk**: Fantastic, John! Let's get that set up for you...\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "prompt = f\"\"\" Your task is to perform the following actions:\n",
    "1 - Summarize the text delimited by triple backticks into single sentence.\n",
    "2 - Translate the summary into Portuguese.\n",
    "3 - List the Customer questions.\n",
    "\n",
    "Use the following output format:\n",
    "Summary: <summary>\n",
    "Translated summary: <portuguese summary>\n",
    "Customer questions: <enumerated customer questions>\n",
    "\n",
    "Text: ```{text}``` <|im_end|>\n",
    "\"\"\"\n",
    "\n",
    "llm=init_llm()\n",
    "sum = llm(prompt)\n",
    "display(Markdown(sum))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Let's do some math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Correct"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = f\"\"\" Determine if the student's solution delimited by the triple backticks is correct or not.\n",
    "Question: When I was 2 years old my sister was twice my age. I'm now 40 years old how old is my sister now? \n",
    "Student's answer: ```The sister is now 80 years old.``` \n",
    "If student is correct, then write 'Correct', otherwise write 'Incorrect'.  \n",
    "<|im_end|>\n",
    "\"\"\"\n",
    "\n",
    "llm=init_llm()\n",
    "sum = llm(prompt)\n",
    "display(HTML(sum))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Ask model to do its own solution and then to compare both solutions and coclude which is correct.\n",
    " Define the task as a list of sub-tasks and ask the model to perform them in a specific order (splitting the task into sub-tasks technique). \n",
    " Then ask the model to compare its own solution with the solution provided by the model and conclude which is correct.\n",
    " Ask model to share it reasoning for the conclusion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Actual solution steps: \n",
       "- When I was 2 years old, my sister was 4 years old (2 * 2 = 4)\n",
       "- The age difference between us is 2 years (4 - 2 = 2)\n",
       "- I am now 40 years old (given)\n",
       "- Therefore, my sister is 40 + 2 = 42 years old now (age difference + my age)\n",
       "Student's solution: The sister is now 80 years old.\n",
       "Student's solution is correct: False"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Question = f\"\"\"When I was 2 years old my sister was twice of my age . I'm now 40 years old, how old is my sister now?\"\"\"\n",
    "Student_Solution = f\"\"\" The sister is now 80 years old.\"\"\"\n",
    "\n",
    "prompt = f\"\"\" Determine if the Student's Solution for the Question is correct or not.\n",
    "To solve the problem do the following:\n",
    "1 - First, work out your OWN solution to the problem. Evaluate your final result to make sure it is correct and adheres to the question's conditions. \n",
    "2 - Second, compare your solution to the student's solution and evaluate if the student's solution is correct or not.\n",
    "Don't decide if the student's solution is correct until you have done the problem yourself.\n",
    "\n",
    "Student's Solution: ```{Student_Solution}```\n",
    "Question: ```{Question}```\n",
    "\n",
    "\n",
    "Use the following output format:\n",
    "Actual solution steps: <your own solution steps>\n",
    "Student's solution: <student's solution>\n",
    "Student's solution is correct: <true/false>\n",
    "<|im_end|>\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "llm=init_llm()\n",
    "sum = llm(prompt)\n",
    "display(Markdown(sum))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Hallucinations and how to avoid them\n",
    "\n",
    "Hallucination is when the model generates text that is not supported by the input.\n",
    "To reduce hallucinations, use the techniques listed above, and strictly instruct the model to find the relevant information in the input text. \n",
    "If the information is not in the input, instruct the model to generate a specific output, e.g. \"I don't know the answer to this question\". "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Iterative Prompt Development\n",
    "Prompt engineering is an iterative process. You almost never get the prompt right the first time.\n",
    "You start with the idea, then you implement the prompt, get the experimental resuslts, do the Error Analysis and iterate again.\n",
    "\n",
    "**Try -> Analyze -> Clarify Instructions -> Try again**\n",
    "\n",
    "In more advanced phases refine prompts with a batch of examples."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Summarization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#helper function\n",
    "#Note that this functions calculates the number of tokens in the prompt\n",
    "def summarize_text(llm, prompt_prefix, text_file):\n",
    "    \n",
    "    # read the text file \n",
    "    with open(text_file, 'r') as file:\n",
    "        text = file.read()\n",
    "    # concatenate the prompt with the data\n",
    "    prompt = prompt_prefix.format(text=text)\n",
    "    return llm(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Imaginal Bank offers a range of investment programs, including the Balanced Growth Fund, Index Tracker ETF, and Secure Income Bond Fund. The Balanced Growth Fund is a diversified mutual fund that invests in a mix of equities and bonds to provide both growth and income, reducing risk through diversification. The bank's portfolio managers actively manage the fund, adjusting holdings based on changing market conditions to manage risk and enhance returns. The Imaginal Investor Dashboard provides real-time tracking of investments, balance updates, and market trends. Customers can schedule an appointment with a financial advisor to help them choose the right investment program based on their financial goals and risk tolerance."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "llm=init_llm()\n",
    "prompt_prefix = \"\"\" Summarize the text delimited by triple backticks into 2-3 sentences: ```{text}``` \n",
    "<|im_end|>\n",
    "\"\"\"\n",
    "\n",
    "sum = summarize_text(llm, prompt_prefix, \"./data/bank-call-center-transcript.txt\")\n",
    "display(Markdown(sum))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### More advanced prompts"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The clerk successfully accomplished the following tasks:\n",
       "\n",
       "- Greeting the customer politely and professionally.\n",
       "- Accurately understanding the customer's inquiry.\n",
       "- Providing clear and detailed information in response.\n",
       "- Asking questions as needed for clarification.\n",
       "- Discussing both benefits and risks with the customer.\n",
       "- Explaining the tools and resources available to the customer.\n",
       "- Inviting the customer to take further action.\n",
       "- Offering assistance for the next steps.\n",
       "- Ending the conversation on a positive note."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "llm=init_llm()\n",
    "prompt_prefix = \"\"\" Prepare a summary for the text delimited by the triple backticks```{text}``` based on the points mentioned below. \n",
    "Please evaluate whether the clerk successfully accomplished the following tasks:\n",
    "\n",
    "Greeting the customer politely and professionally.\n",
    "Accurately understanding the customer's inquiry.\n",
    "Providing clear and detailed information in response.\n",
    "Asking questions as needed for clarification.\n",
    "Discussing both benefits and risks with the customer.\n",
    "Explaining the tools and resources available to the customer.\n",
    "Inviting the customer to take further action.\n",
    "Offering assistance for the next steps.\n",
    "Ending the conversation on a positive note.\n",
    "\n",
    "<|im_end|>\n",
    "\"\"\"\n",
    "\n",
    "sum = summarize_text(llm, prompt_prefix, \"./data/bank-call-center-transcript.txt\")\n",
    "display(Markdown(sum))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Output in tabular format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <td>Did the clerk greet the customer in a polite and professional manner?</td>\n",
       "        <td style=\"color:green\">Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>Did the clerk begin the conversation on a negative note?</td>\n",
       "        <td style=\"color:green\">No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>Did the clerk understand the customer's inquiry?</td>\n",
       "        <td style=\"color:green\">Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>Did the clerk provide clear and detailed information?</td>\n",
       "        <td style=\"color:green\">Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>Did the clerk ask questions to clarify the situation?</td>\n",
       "        <td style=\"color:green\">Yes</td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "llm=init_llm()\n",
    "prompt_prefix = \"\"\"Prepare a summary for the text delimited by the triple backticks:```{text}``` based on the points mentioned below.\n",
    "Generate the output in the HTML format, with each item on a separate row.\n",
    "\n",
    "Assign a color code to each item based on the clerk's performance - \n",
    "items that were successfully addressed should be marked in green, \n",
    "whereas items that were not met should be highlighted in red.\n",
    "\n",
    "Here are the evaluation criterias to consider:\n",
    "\n",
    "Did the clerk greet the customer in a polite and professional manner?\n",
    "Did the clerk begin the conversation on a negative note?\n",
    "Did the clerk understand the customer's inquiry?\n",
    "Did the clerk provide clear and detailed information?\n",
    "Did the clerk ask questions to clarify the situation?\n",
    "\n",
    "\n",
    "<|im_end|>\n",
    "\"\"\"\n",
    "\n",
    "res = summarize_text(llm, prompt_prefix, \"./data/bank-call-center-transcript.txt\")\n",
    "display(HTML(res))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Output as JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "Ask Clarifying Questions": "Yes",
       "Discuss the Benefits and Risks": "Yes",
       "End on a Positive Note": "Yes",
       "Explain Available Tools and Resources": "Yes",
       "Greet the Customer Politely and Professionally": "Yes",
       "Invite Further Action": "Yes",
       "Offer to Assist with Next Steps": "Yes",
       "Provide Clear and Detailed Information": "Yes",
       "Understand the Customer's Inquiry": "Yes"
      },
      "text/plain": [
       "<IPython.core.display.JSON object>"
      ]
     },
     "metadata": {
      "application/json": {
       "expanded": false,
       "root": "root"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "llm=init_llm()\n",
    "\n",
    "prompt_prefix = \"\"\"\" Evaluate the clerk performance from the text delimited by the triple backticks: ```{text}``` based on the points mentioned below.\n",
    "\n",
    "Did the clerk greet the customer in a polite and professional manner?\n",
    "Did the clerk comprehend the customer's inquiry accurately?\n",
    "Did the clerk provide comprehensive and clear information?\n",
    "Did the clerk ask relevant questions to clarify the customer's situation?\n",
    "Did the clerk explain the benefits and potential risks to the customer?\n",
    "Did the clerk detail the tools and resources available to the customer?\n",
    "Did the clerk encourage the customer to take further action?\n",
    "Did the clerk offer assistance with proceeding to the next steps?\n",
    "Did the clerk end the interaction on a positive and upbeat note?\"\n",
    "\n",
    "The output should be presented in JSON format, adhering to the following key-value pairs:\n",
    "\n",
    "\"Greet the Customer Politely and Professionally\": \"Yes/No\",\n",
    "\"Understand the Customer's Inquiry\": \"Yes/No\",\n",
    "\"Provide Clear and Detailed Information\": \"Yes/No\",\n",
    "\"Ask Clarifying Questions\": \"Yes/No\",\n",
    "\"Discuss the Benefits and Risks\": \"Yes/No\",\n",
    "\"Explain Available Tools and Resources\": \"Yes/No\",\n",
    "\"Invite Further Action\": \"Yes/No\",\n",
    "\"Offer to Assist with Next Steps\": \"Yes/No\",\n",
    "\"End on a Positive Note\": \"Yes/No\"\n",
    "\n",
    "<|im_end|>\n",
    "\"\"\"\n",
    "\n",
    "res = summarize_text(llm, prompt_prefix, \"./data/bank-call-center-transcript.txt\")\n",
    "display(JSON(json.loads(res)))\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarize large documents\n",
    "To summarize larger documents, we can split the document into smaller chunks and summarize each chunk separately. We can then combine the summaries of each chunk to get the final summary.\n",
    "LangChain has a built-in chain for doing that. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import OpenAI\n",
    "from langchain.chains.summarize import load_summarize_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens in the document: 13420\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "llm=init_llm()\n",
    "\n",
    "large_pdf_path =\"./data/Large_language_model.pdf\"\n",
    "loader = PyPDFLoader(large_pdf_path)\n",
    "\n",
    "#output type is List[Document]\n",
    "pages = loader.load_and_split()\n",
    "\n",
    "#count tokens in the document\n",
    "total_tokens = 0\n",
    "encoder = enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "for page in pages:\n",
    "    total_tokens += len(encoder.encode(page.page_content))\n",
    "    \n",
    "# The document has more 13000 tokens. This is too many for the LLM to process in one go.\n",
    "# This is whay we split the document into smaller chunks.\n",
    "print(f\"Total tokens in the document: {total_tokens}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create summarization chain\n",
    "summary_chain = load_summarize_chain(llm=llm, chain_type='map_reduce',verbose=False )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### There are following chain_types: stuff, map_reduce, refine, map-rerank\n",
    "See here for the details: https://docs.langchain.com/docs/components/chains/index_related_chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       " \n",
       "\n",
       "The article discusses the use of language models (LLMs) in natural language processing (NLP) tasks. It explains zero-shot prompting, where the model is given no solved examples, and few-shot performance, where the model is given a small number of solved examples. The article also discusses instruction tuning, a form of fine-tuning that trains the LLM on many examples of tasks formulated as natural language instructions. The article explains various techniques for instruction tuning, including self-instruct and OpenAI's InstructGPT protocol. The article concludes by discussing perplexity, a measure of how well a model is able to predict the contents of a dataset, and the challenges of evaluating large language models. \n",
       "\n",
       "\"\"\"<|im_sep|>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Many calls in loop causing RateLimitError: Too Many Requests\n",
    "#sum = summary_chain.run(pages)\n",
    "sum = summary_chain.run(pages[0:5])\n",
    "\n",
    "display(Markdown(sum))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

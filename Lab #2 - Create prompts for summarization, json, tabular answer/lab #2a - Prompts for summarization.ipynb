{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Load Environment variables from .env file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain.llms import AzureOpenAI\n",
    "import openai\n",
    "import tiktoken\n",
    "from IPython.display import display, HTML, JSON, Markdown\n",
    "# to avoid error: Could not automatically map gpt-35-turbo to a tokeniser...\n",
    "tiktoken.model.MODEL_TO_ENCODING[\"gpt-35-turbo\"] = \"cl100k_base\"\n",
    "tiktoken.model.MODEL_TO_ENCODING[\"gpt-35-turbo-instruct\"] = \"cl100k_base\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "load_dotenv()\n",
    "# env variables that are used by LangChain\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv(\"OPENAI_API_KEY\")\n",
    "os.environ['OPENAI_API_TYPE'] = \"azure\"\n",
    "os.environ['OPENAI_API_VERSION'] = os.getenv(\"OPENAI_DEPLOYMENT_VERSION\")\n",
    "os.environ['OPENAI_API_BASE'] = os.getenv(\"OPENAI_DEPLOYMENT_ENDPOINT\")\n",
    "\n",
    "OPENAI_DEPLOYMENT_ENDPOINT = os.getenv(\"OPENAI_DEPLOYMENT_ENDPOINT\")\n",
    "OPENAI_DEPLOYMENT_NAME = os.getenv(\"OPENAI_DEPLOYMENT_NAME\")\n",
    "OPENAI_MODEL_NAME = os.getenv(\"OPENAI_MODEL_NAME\")\n",
    "\n",
    "OPENAI_ADA_EMBEDDING_DEPLOYMENT_NAME = os.getenv(\n",
    "    \"OPENAI_ADA_EMBEDDING_DEPLOYMENT_NAME\")\n",
    "OPENAI_ADA_EMBEDDING_MODEL_NAME = os.getenv(\"OPENAI_ADA_EMBEDDING_MODEL_NAME\")\n",
    "\n",
    "#gpt-35-turbo-instruct\n",
    "OPENAI_GPT_35_TURBO_INSTRUCT_MODEL_NAME=\"gpt-35-turbo-instruct\"\n",
    "OPENAI_GPT_35_TURBO_INSTRUCT_DEPLOYMENT_NAME=\"gpt-35-turbo-instruct\"\n",
    "\n",
    "# Configure OpenAI API\n",
    "openai.api_type = \"azure\"\n",
    "openai.api_version = os.getenv(\"OPENAI_DEPLOYMENT_VERSION\")\n",
    "openai.api_base = os.getenv(\"OPENAI_DEPLOYMENT_ENDPOINT\")\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Model initialization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def init_llm(model=OPENAI_GPT_35_TURBO_INSTRUCT_MODEL_NAME,\n",
    "             deployment_name=OPENAI_GPT_35_TURBO_INSTRUCT_DEPLOYMENT_NAME,\n",
    "             temperature=0,\n",
    "             max_tokens=400,\n",
    "             top_p=1,\n",
    "             ):\n",
    "\n",
    "    llm = AzureOpenAI(deployment_name=deployment_name,\n",
    "                      model=model,\n",
    "                      temperature=temperature,\n",
    "                      max_tokens=max_tokens,\n",
    "                      top_p=top_p,\n",
    "                      model_kwargs={\"stop\": [\"<|im_end|>\"]}\n",
    "                      )\n",
    "    return llm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### **Prompt engineering techniques**\n",
    "Those techniques are not specific to summarization, but can be used for any task.\n",
    "\n",
    "1. Use delimeters to clearly separate the exact text the model should summarize.\n",
    "\n",
    "    Delimiters could be any kind of punctuation, that separates specific pieces of text. \n",
    "    Tripple quotes, Triple backtics, Triple dashes, Angle brackets, XML tags, etc.\n",
    "\n",
    "2. Ask model for structured output, e.g json, html, etc. If you ask for a json define the structure of the json, e.g. what fields should be in the json.\n",
    "\n",
    "3. Check assumptions required to do the task.\n",
    "\n",
    "4. Few-shot prompting. Provide examples of completing tasks and then ask model to perform the task.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Input text\n",
    "\n",
    "text = f\"\"\"\n",
    "**Customer**: Hello, my name is John, and I'm a customer of Imaginal Bank.\n",
    "**Clerk**: Hello, John! My name is Sara, and I'm a customer service representative at Imaginal Bank. How can I assist you today?\n",
    "**Customer**: Hi, Sara. I'm interested in your bank's investment programs. \n",
    "              Can you tell me more about them, especially in terms of risk management?\n",
    "\n",
    "**Clerk**: Absolutely, John. We have a few key programs I can highlight.\n",
    "\n",
    "First, there's our 'Balanced Growth Fund'. It's a diversified mutual fund that invests in a mix of equities and bonds to provide both growth and income, reducing risk through diversification. \n",
    "\n",
    "We also have the 'Index Tracker ETF', which is designed to replicate the performance of a specific market index. By spreading investments across the entire index, it inherently reduces the risk associated with individual stocks.\n",
    "\n",
    "Additionally, for those with a lower risk tolerance, we have the 'Secure Income Bond Fund', which focuses on government and high-quality corporate bonds. \n",
    "\n",
    "Our financial advisors are always available to guide you in choosing the right program based on your financial goals and risk tolerance.\n",
    "\n",
    "**Customer**: I see. Could you elaborate on how the Balanced Growth Fund manages risk?\n",
    "\n",
    "**Clerk**: Sure. The Balanced Growth Fund mitigates risk by diversifying investments across a wide range of assets. If one investment performs poorly, it's likely to be offset by other investments that are performing well. Furthermore, our portfolio managers actively manage the fund, adjusting holdings based on changing market conditions to manage risk and enhance returns.\n",
    "\n",
    "**Customer**: Does the bank provide any tools to monitor my investments?\n",
    "\n",
    "**Clerk**: Yes, John. We offer an online platform called 'Imaginal Investor Dashboard'. It provides real-time tracking of your investments, balance updates, and market trends. You can also set up alerts to be notified about significant changes in your portfolio.\n",
    "\n",
    "**Customer**: That sounds quite comprehensive. How can I get started?\n",
    "\n",
    "**Clerk**: You can schedule an appointment with one of our financial advisors. They'll walk you through your options, help you understand your risk tolerance, and guide you in choosing the right investment program. Would you like me to arrange that for you?\n",
    "\n",
    "**Customer**: Yes, please. That would be helpful.\n",
    "\n",
    "**Clerk**: Fantastic, John! Let's get that set up for you...\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Summary: Imaginal Bank offers various investment programs with risk management strategies, including a diversified mutual fund, index tracker ETF, and secure income bond fund. They also provide an online platform for monitoring investments and offer guidance from financial advisors."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Use delimiters to clearly separate input text\n",
    "\n",
    "prompt = f\"\"\" Summarize the text delimited by triple backticks into a summary of 20 words. \n",
    "```{text}```\n",
    "\n",
    "Output the summary in the form: Summary: <summary> <|im_end|>\n",
    "\"\"\"\n",
    "\n",
    "#init model gpt-35-turbo-instruct, which supports Completion API\n",
    "llm = init_llm()\n",
    "\n",
    "sum = llm(prompt)\n",
    "display(Markdown(sum))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AzureOpenAI(cache=None, verbose=False, callbacks=None, callback_manager=None, tags=None, metadata=None, client=<class 'openai.api_resources.completion.Completion'>, model_name='gpt-35-turbo-instruct', temperature=0.0, max_tokens=400, top_p=1.0, frequency_penalty=0, presence_penalty=0, n=1, best_of=1, model_kwargs={'stop': ['<|im_end|>']}, openai_api_key='aa87dc8868f34c15b8c299c036c347b6', openai_api_base='https://openailabs303474.openai.azure.com/', openai_organization='', openai_proxy='', batch_size=20, request_timeout=None, logit_bias={}, max_retries=6, streaming=False, allowed_special=set(), disallowed_special='all', tiktoken_model_name=None, deployment_name='gpt-35-turbo-instruct', openai_api_type='azure', openai_api_version='2023-05-15')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check llm object content. Pay attention that openai.api_resources.completion.Completion\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Summary: The customer, John, inquires about investment programs at Imaginal Bank and the clerk, Sara, explains the Balanced Growth Fund, Index Tracker ETF, and Secure Income Bond Fund as options for managing risk. The clerk also mentions the Imaginal Investor Dashboard for monitoring investments and offers to schedule an appointment with a financial advisor to assist John in choosing the right program based on his risk tolerance."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check the specific conditions/assumptions in the input text\n",
    "\n",
    "prompt = f\"\"\" You will be provided with a text delimited by triple quotes. \n",
    "If it contains utterances of a Customer and a Clerk, summarize the text into single sentence.\n",
    "If the text does not contain utterances of a Customer and a Clerk, \n",
    "then just write 'No customer and clerk utterances found'.\n",
    "\\\"\\\"\\\" {text}  \\\"\\\"\\\"\n",
    "\n",
    "Output a summary in the form: Summary: <summary> \n",
    "<|im_end|>\n",
    "\"\"\"\n",
    "llm = init_llm()\n",
    "sum = llm(prompt)\n",
    "display(Markdown(sum))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Give the model time to think. Chain of Thoughts.\n",
    "\n",
    "Very useful technique is to give the model time to think by specifying the strict steps (sub-tasks) to complete the task and asking for output in a specific format. This is a very powerful technique.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Summary: The customer, John, is interested in Imaginal Bank's investment programs and asks the clerk, Sara, about them, specifically in terms of risk management. Sara explains the different programs, including the Balanced Growth Fund, Index Tracker ETF, and Secure Income Bond Fund, and mentions that financial advisors are available to help choose the right program based on financial goals and risk tolerance. John also asks about how the Balanced Growth Fund manages risk, and Sara explains the diversification and active management by portfolio managers. John also asks about tools to monitor investments, and Sara mentions the Imaginal Investor Dashboard. Finally, John asks how to get started and Sara offers to schedule an appointment with a financial advisor.\n",
       "\n",
       "Translated summary: O cliente, John, está interessado nos programas de investimento do Imaginal Bank e pergunta à atendente, Sara, sobre eles, especificamente em termos de gerenciamento de risco. Sara explica os diferentes programas, incluindo o Balanced Growth Fund, Index Tracker ETF e Secure Income Bond Fund, e menciona que os consultores financeiros estão disponíveis para ajudar a escolher o programa certo com base nos objetivos financeiros e tolerância ao risco. John também pergunta sobre como o Balanced Growth Fund gerencia o risco, e Sara explica a diversificação e o gerenciamento ativo pelos gestores de portfólio. John também pergunta sobre ferramentas para monitorar investimentos, e Sara menciona o Imaginal Investor Dashboard. Finalmente, John pergunta como começar e Sara oferece para agendar uma consulta com um consultor financeiro.\n",
       "\n",
       "Customer questions: \n",
       "1. Can you tell me more about Imaginal Bank's investment programs, especially in terms of risk management? \n",
       "2. How does the Balanced Growth Fund manage risk? \n",
       "3. Does the bank provide any tools to monitor investments? \n",
       "4. How can I get started?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = f\"\"\" Your task is to perform the following actions:\n",
    "1 - Summarize the text delimited by triple backticks into single sentence.\n",
    "2 - Translate the summary into Portuguese.\n",
    "3 - List the Customer questions.\n",
    "\n",
    "Use the following output format:\n",
    "Summary: <summary>\n",
    "Translated summary: <portuguese summary>\n",
    "Customer questions: <enumerated customer questions>\n",
    "\n",
    "Text: ```{text}``` <|im_end|>\n",
    "\"\"\"\n",
    "\n",
    "llm = init_llm()\n",
    "sum = llm(prompt)\n",
    "display(Markdown(sum))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Let's do some math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Incorrect. The sister is now 38 years old. This can be determined by subtracting 2 from the current age of the student (40) and then dividing by 2. This gives us the age of the sister when the student was 2 years old, which is 38."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = f\"\"\" Determine if the student's solution delimited by the triple backticks is correct or not.\n",
    "Question: When I was 2 years old, my sister was twice my age. I'm now 40 years old. How old is my sister now? \n",
    "Student's answer: ```The sister is now 80 years old.``` \n",
    "If the student answer is correct, write 'Correct', otherwise write 'Incorrect'. Explain your answer.  \n",
    "<|im_end|>\n",
    "\"\"\"\n",
    "\n",
    "llm = init_llm()\n",
    "sum = llm(prompt)\n",
    "display(HTML(sum))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Ask model to do its own solution and then to compare both solutions and conclude which is correct.\n",
    " Define the task as a list of sub-tasks and ask the model to perform them in a specific order (splitting the task into sub-tasks technique). \n",
    " Then ask the model to compare its own solution with the solution provided by the model and conclude which is correct.\n",
    " Ask model to share it reasoning for the conclusion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Actual solution steps:\n",
       "1 - First, work out your OWN solution to the problem. \n",
       "    Let x be the sister's age when I was 2 years old.\n",
       "    Then, my age at that time was 2 years old.\n",
       "    Since my sister was twice my age, we can write the equation:\n",
       "    x = 2 * 2 = 4\n",
       "    Therefore, my sister was 4 years old when I was 2 years old.\n",
       "2 - Second, we can use the fact that I am now 40 years old to find my sister's current age.\n",
       "    Since I am 40 years old, my sister is 40 - 2 = 38 years older than me.\n",
       "    Therefore, my sister's current age is 4 + 38 = 42 years old.\n",
       "\n",
       "Student's solution: The sister is now 80 years old.\n",
       "\n",
       "Student's solution is correct: False"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Question = f\"\"\"When I was 2 years old my sister was twice of my age . I'm now 40 years old, how old is my sister now?\"\"\"\n",
    "Student_Solution = f\"\"\" The sister is now 80 years old.\"\"\"\n",
    "\n",
    "prompt = f\"\"\" Determine if the Student's Solution for the Question is correct or not.\n",
    "To solve the problem do the following:\n",
    "1 - First, work out your OWN solution to the problem. Evaluate your final result to make sure it is correct and adheres to the question's conditions. \n",
    "    Reason about every step of your solution and make sure it is correct. \n",
    "2 - Second, compare your solution to the student's solution and evaluate if the student's solution is correct or not.\n",
    "Don't decide if the student's solution is correct until you have done the problem yourself.\n",
    "\n",
    "Student's Solution: ```{Student_Solution}```\n",
    "Question: ```{Question}```\n",
    "\n",
    "\n",
    "Use the following output format:\n",
    "Actual solution steps: <your own solution steps>\n",
    "\\nStudent's solution: <student's solution>\n",
    "\\nStudent's solution is correct: <true/false>\n",
    "<|im_end|>\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "llm = init_llm()\n",
    "sum = llm(prompt)\n",
    "display(Markdown(sum))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Hallucinations and how to avoid them\n",
    "\n",
    "Hallucination is when the model generates text that is not supported by the input.\n",
    "To reduce hallucinations, use the techniques listed above, and strictly instruct the model to find the relevant information in the input text. \n",
    "If the information is not in the input, instruct the model to generate a specific output, e.g. \"I don't know the answer to this question\". "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Iterative Prompt Development\n",
    "Prompt engineering is an iterative process. You almost never get the prompt right the first time.\n",
    "You start with the idea, then you implement the prompt, get the experimental resuslts, do the Error Analysis and iterate again.\n",
    "\n",
    "**Try -> Analyze -> Clarify Instructions -> Try again**\n",
    "\n",
    "In more advanced phases refine prompts with a batch of examples."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function\n",
    "def summarize_text(llm, prompt_prefix, text_file):\n",
    "\n",
    "    # read the text file\n",
    "    with open(text_file, 'r') as file:\n",
    "        text = file.read()\n",
    "    # concatenate the prompt with the data\n",
    "    prompt = prompt_prefix.format(text=text)\n",
    "    return llm(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "John, a customer of Imaginal Bank, is interested in their investment programs and asks Sara, a customer service representative, about them. Sara explains that they have a Balanced Growth Fund, an Index Tracker ETF, and a Secure Income Bond Fund, each with different levels of risk management. She also mentions their online platform, Imaginal Investor Dashboard, for monitoring investments. John decides to schedule an appointment with a financial advisor to get started."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "llm = init_llm()\n",
    "prompt_prefix = \"\"\" Summarize the text delimited by triple backticks into 2-3 sentences: ```{text}``` \n",
    "<|im_end|>\n",
    "\"\"\"\n",
    "\n",
    "sum = summarize_text(llm, prompt_prefix,\n",
    "                     \"./data/bank-call-center-transcript.txt\")\n",
    "display(Markdown(sum))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### More advanced prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The clerk successfully accomplished all of the mentioned tasks. They greeted the customer politely and professionally, accurately understood the customer's inquiry, and provided clear and detailed information in response. They also asked questions for clarification and discussed both the benefits and risks of the investment programs. The clerk explained the tools and resources available to the customer and invited them to take further action by scheduling an appointment with a financial advisor. They also offered assistance for the next steps and ended the conversation on a positive note. Overall, the clerk effectively assisted the customer and provided them with the necessary information to make an informed decision about their investments."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "llm = init_llm()\n",
    "prompt_prefix = \"\"\" Prepare a summary for the text delimited by the triple backticks```{text}``` based on the points mentioned below. \n",
    "Please evaluate whether the clerk successfully accomplished the following tasks:\n",
    "\n",
    "Greeting the customer politely and professionally.\n",
    "Accurately understanding the customer's inquiry.\n",
    "Providing clear and detailed information in response.\n",
    "Asking questions as needed for clarification.\n",
    "Discussing both benefits and risks with the customer.\n",
    "Explaining the tools and resources available to the customer.\n",
    "Inviting the customer to take further action.\n",
    "Offering assistance for the next steps.\n",
    "Ending the conversation on a positive note.\n",
    "\n",
    "<|im_end|>\n",
    "\"\"\"\n",
    "\n",
    "sum = summarize_text(llm, prompt_prefix,\n",
    "                     \"./data/bank-call-center-transcript.txt\")\n",
    "display(Markdown(sum))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Output in tabular format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "  <tr>\n",
       "    <th>Customer</th>\n",
       "    <th>Clerk</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td>Hello, my name is John, and I'm a customer of Imaginal Bank.</td>\n",
       "    <td style=\"color: green;\">Hello, John! My name is Sara, and I'm a customer service representative at Imaginal Bank. How can I assist you today?</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td>Hi, Sara. I'm interested in your bank's investment programs. Can you tell me more about them, especially in terms of risk management?</td>\n",
       "    <td style=\"color: green;\">Absolutely, John. We have a few key programs I can highlight.</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td></td>\n",
       "    <td style=\"color: green;\">The Balanced Growth Fund mitigates risk by diversifying investments across a wide range of assets. If one investment performs poorly, it's likely to be offset by other investments that are performing well. Furthermore, our portfolio managers actively manage the fund, adjusting holdings based on changing market conditions to manage risk and enhance returns.</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td></td>\n",
       "    <td style=\"color: green;\">The Secure Income Bond Fund focuses on government and high-quality corporate bonds, making it a lower risk option for those with a lower risk tolerance.</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td></td>\n",
       "    <td style=\"color: green;\">Our financial advisors are always available to guide you in choosing the right program based on your financial goals and risk tolerance.</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td></td>\n",
       "    <td style=\"color: green;\">You can schedule an appointment with one of our financial advisors. They'll walk you through your options, help you understand your risk tolerance, and guide"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "llm = init_llm()\n",
    "prompt_prefix = \"\"\"Prepare a summary for the text delimited by the triple backticks:```{text}``` based on the points mentioned below.\n",
    "Generate the output as a HTML table, with 2 columns: Customer and Clerk.\n",
    "\n",
    "Assign a color code to each item based on the clerk's performance - \n",
    "items that were successfully addressed should be marked in green, \n",
    "whereas items that were not met should be highlighted in red.\n",
    "\n",
    "Here are the evaluation criterias to consider:\n",
    "\n",
    "Did the clerk greet the customer in a polite and professional manner?\n",
    "Did the clerk begin the conversation on a negative note?\n",
    "Did the clerk understand the customer's inquiry?\n",
    "Did the clerk provide clear and detailed information?\n",
    "Did the clerk ask questions to clarify the situation?\n",
    "\n",
    "\n",
    "<|im_end|>\n",
    "\"\"\"\n",
    "\n",
    "res = summarize_text(llm, prompt_prefix,\n",
    "                     \"./data/bank-call-center-transcript.txt\")\n",
    "display(HTML(res))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Output as JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "Ask Clarifying Questions": "Yes",
       "Discuss the Benefits and Risks": "Yes",
       "End on a Positive Note": "Yes",
       "Explain Available Tools and Resources": "Yes",
       "Greet the Customer Politely and Professionally": "Yes",
       "Invite Further Action": "Yes",
       "Offer to Assist with Next Steps": "Yes",
       "Provide Clear and Detailed Information": "Yes",
       "Understand the Customer's Inquiry": "Yes"
      },
      "text/plain": [
       "<IPython.core.display.JSON object>"
      ]
     },
     "metadata": {
      "application/json": {
       "expanded": false,
       "root": "root"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "llm = init_llm()\n",
    "\n",
    "prompt_prefix = \"\"\"\" Evaluate the clerk performance from the text delimited by the triple backticks: ```{text}``` based on the points mentioned below.\n",
    "\n",
    "Did the clerk greet the customer in a polite and professional manner?\n",
    "Did the clerk comprehend the customer's inquiry accurately?\n",
    "Did the clerk provide comprehensive and clear information?\n",
    "Did the clerk ask relevant questions to clarify the customer's situation?\n",
    "Did the clerk explain the benefits and potential risks to the customer?\n",
    "Did the clerk detail the tools and resources available to the customer?\n",
    "Did the clerk encourage the customer to take further action?\n",
    "Did the clerk offer assistance with proceeding to the next steps?\n",
    "Did the clerk end the interaction on a positive and upbeat note?\"\n",
    "\n",
    "The output should be presented in JSON format, adhering to the following key-value pairs:\n",
    "\n",
    "\"Greet the Customer Politely and Professionally\": \"Yes/No\",\n",
    "\"Understand the Customer's Inquiry\": \"Yes/No\",\n",
    "\"Provide Clear and Detailed Information\": \"Yes/No\",\n",
    "\"Ask Clarifying Questions\": \"Yes/No\",\n",
    "\"Discuss the Benefits and Risks\": \"Yes/No\",\n",
    "\"Explain Available Tools and Resources\": \"Yes/No\",\n",
    "\"Invite Further Action\": \"Yes/No\",\n",
    "\"Offer to Assist with Next Steps\": \"Yes/No\",\n",
    "\"End on a Positive Note\": \"Yes/No\"\n",
    "\n",
    "<|im_end|>\n",
    "\"\"\"\n",
    "\n",
    "res = summarize_text(llm, prompt_prefix,\n",
    "                     \"./data/bank-call-center-transcript.txt\")\n",
    "display(JSON(json.loads(res)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarize large documents\n",
    "To summarize larger documents, we can split the document into smaller chunks and summarize each chunk separately. We can then combine the summaries of each chunk to get the final summary.\n",
    "LangChain has a built-in chain for doing that. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens in the document: 13420\n"
     ]
    }
   ],
   "source": [
    "from langchain import OpenAI\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "llm = init_llm()\n",
    "\n",
    "large_pdf_path = \"./data/Large_language_model.pdf\"\n",
    "loader = PyPDFLoader(large_pdf_path)\n",
    "\n",
    "# output type is List[Document]\n",
    "pages = loader.load_and_split()\n",
    "\n",
    "# count tokens in the document\n",
    "total_tokens = 0\n",
    "encoder = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "for page in pages:\n",
    "    total_tokens += len(encoder.encode(page.page_content))\n",
    "\n",
    "# The document has more 13000 tokens. This is too many for the LLM to process in one go.\n",
    "# This is whay we split the document into smaller chunks.\n",
    "print(f\"Total tokens in the document: {total_tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.summarize import load_summarize_chain\n",
    "\n",
    "# create summarization chain\n",
    "# map_reduce summarizes each document on it's own in a \"map\" step and then \"reduce\" the summaries into a final summary\n",
    "summary_chain = load_summarize_chain( \n",
    "    llm=llm, chain_type='map_reduce', verbose=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### There are following chain_types: stuff, map_reduce, refine, map-rerank\n",
    "See here for the details: https://docs.langchain.com/docs/components/chains/index_related_chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       " This article discusses various large language models, their capabilities, and their potential applications. It also covers the emergence of new models and techniques, such as instruction fine-tuning and prompting, and the ethical considerations surrounding these models. The article also mentions the upcoming release of several new models and their potential uses in different industries."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Many calls in loop causing RateLimitError: Too Many Requests. In this sum = summary_chain.run(pages[0:10]) \n",
    "sum = summary_chain.run(pages)\n",
    "\n",
    "display(Markdown(sum))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO - Upgrade LangChain version to the latest\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.chains.mapreduce import MapReduceChain\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.chains import ReduceDocumentsChain, MapReduceDocumentsChain, StuffDocumentsChain\n",
    "\n",
    "llm = init_llm()\n",
    "\n",
    "# Map\n",
    "map_template = \"\"\"The following is a set of documents\n",
    "{docs}\n",
    "Based on this list of docs, please identify the main themes \n",
    "Helpful Answer:\n",
    "<|im_end|>\"\"\"\n",
    "\n",
    "map_prompt = PromptTemplate.from_template(map_template)\n",
    "map_chain = LLMChain(llm=llm, prompt=map_prompt)\n",
    "\n",
    "# Reduce\n",
    "reduce_template = \"\"\"The following is set of summaries:\n",
    "{doc_summaries}\n",
    "Take these and distill it into a final, consolidated summary of the main themes. \n",
    "Summary:\n",
    "\n",
    "<|im_end|>\n",
    "\"\"\"\n",
    "reduce_prompt = PromptTemplate.from_template(reduce_template)\n",
    "reduce_chain = LLMChain(llm=llm, prompt=reduce_prompt)\n",
    "\n",
    "# Takes a list of documents, combines them into a single string, and passes this to an LLMChain\n",
    "combine_documents_chain = StuffDocumentsChain(\n",
    "    llm_chain=reduce_chain, document_variable_name=\"doc_summaries\"\n",
    ")\n",
    "\n",
    "# Combines and iteravely reduces the mapped documents\n",
    "reduce_documents_chain = ReduceDocumentsChain(\n",
    "    # This is final chain that is called.\n",
    "    combine_documents_chain=combine_documents_chain,\n",
    "    # If documents exceed context for `StuffDocumentsChain`\n",
    "    collapse_documents_chain=combine_documents_chain,\n",
    "    # The maximum number of tokens to group documents into.\n",
    "    token_max=2000,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining documents by mapping a chain over them, then combining results\n",
    "map_reduce_chain = MapReduceDocumentsChain(\n",
    "    # Map chain\n",
    "    llm_chain=map_chain,\n",
    "    # Reduce chain\n",
    "    reduce_documents_chain=reduce_documents_chain,\n",
    "    # The variable name in the llm_chain to put the documents in\n",
    "    document_variable_name=\"docs\",\n",
    "    # Return the results of the map steps in the output\n",
    "    return_intermediate_steps=False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Large language models have become powerful tools in natural language processing, with their capabilities and performance constantly improving. However, concerns about training costs and potential biases remain. Pre-training and fine-tuning techniques are used to optimize these models for specific tasks. Open-source models and collaborations have made these models more accessible, leading to advancements in dialogue and conversational AI. Ethical considerations and responsible AI are also important factors in their development and use. The release of new models, partnerships, and advancements in training data and computing are driving the future of AI and language models, with potential for impact in various industries.'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "map_reduce_chain.run(pages)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

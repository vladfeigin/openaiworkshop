{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import AzureOpenAI\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from IPython.display import display, HTML, JSON\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\") \n",
    "OPENAI_DEPLOYMENT_ENDPOINT = os.getenv(\"OPENAI_DEPLOYMENT_ENDPOINT\")\n",
    "OPENAI_DEPLOYMENT_NAME = os.getenv(\"OPENAI_DEPLOYMENT_NAME\")\n",
    "OPENAI_MODEL_NAME = os.getenv(\"OPENAI_MODEL_NAME\")\n",
    "OPENAI_EMBEDDING_DEPLOYMENT_NAME = os.getenv(\"OPENAI_EMBEDDING_DEPLOYMENT_NAME\")\n",
    "OPENAI_EMBEDDING_MODEL_NAME = os.getenv(\"OPENAI_EMBEDDING_MODEL_NAME\")\n",
    "OPENAI_DEPLOYMENT_VERSION = os.getenv(\"OPENAI_DEPLOYMENT_VERSION\")\n",
    "\n",
    "# Configure OpenAI API\n",
    "openai.api_type = \"azure\"\n",
    "openai.api_version = OPENAI_DEPLOYMENT_VERSION\n",
    "openai.api_base = OPENAI_DEPLOYMENT_ENDPOINT\n",
    "openai.api_key = OPENAI_API_KEY"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_llm(model=\"text-davinci-003\",\n",
    "             deployment_name=\"text-davinci-003\", \n",
    "             temperature=0,\n",
    "             max_tokens=3000,\n",
    "             stop=\"<|im_end|>\", \n",
    "             ):\n",
    "    \n",
    "    llm = AzureOpenAI(deployment_name=deployment_name,  \n",
    "                  model=model,\n",
    "                  temperature=temperature,) \n",
    "    return llm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Text Summarization**\n",
    "\n",
    "There are two types of text summarization: **extractive** and **abstractive**. Extractive summarization involves selecting phrases from the source text and concatenating them to make a summary. In contrast, abstractive summarization aims to generate new phrases and sentences that capture the salient information from the source text. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic Summarization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_text(llm, prompt_prefix, text_file):\n",
    "    \n",
    "    with open(text_file, 'r') as file:\n",
    "        # read the entire file into a string\n",
    "        data = file.read()\n",
    "\n",
    "    # concatenate the prompt with the data\n",
    "    prompt = prompt_prefix + data\n",
    "    #check the number of tokens in the prompt\n",
    "    num_tokens = llm.get_num_tokens(prompt)\n",
    "    print (f\"Number of tokens in final prompt is: {num_tokens}\")\n",
    "    return llm(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm=init_llm()\n",
    "prompt_prefix = \"Summarize the text below for call center supervisor:\\n\"\n",
    "\n",
    "sum = summarize_text(llm, prompt_prefix, \"./data/bank-call-center-transcript.txt\")\n",
    "display(HTML(sum))\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### More advanced prompts"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens in final prompt is: 663\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "The customer service representative successfully greeted the customer politely and professionally, accurately understood the customer's inquiry, provided clear and detailed information in response, asked questions as needed for clarification, discussed both benefits and risks with the customer, explained the tools and resources available to the customer, invited the customer to take further action, offered assistance for the next steps, and ended the conversation on a positive note."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "llm=init_llm()\n",
    "prompt_prefix = \"\"\" Prepare a summary for the call center supervisor based on the points mentioned below. \n",
    "Please evaluate whether the clerk successfully accomplished the following tasks:\n",
    "\n",
    "Greeting the customer politely and professionally.\n",
    "Accurately understanding the customer's inquiry.\n",
    "Providing clear and detailed information in response.\n",
    "Asking questions as needed for clarification.\n",
    "Discussing both benefits and risks with the customer.\n",
    "Explaining the tools and resources available to the customer.\n",
    "Inviting the customer to take further action.\n",
    "Offering assistance for the next steps.\n",
    "Ending the conversation on a positive note.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "sum = summarize_text(llm, prompt_prefix, \"./data/bank-call-center-transcript.txt\")\n",
    "display(HTML(sum))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Output in tabular format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens in final prompt is: 695\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "  <tr>\n",
       "    <th>Evaluation Criteria</th>\n",
       "    <th>Performance</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td>Did the clerk greet the customer in a polite and professional manner?</td>\n",
       "    <td style=\"background-color:green;\">Yes</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td>Did the clerk began the conversation on a negative note?</td>\n",
       "    <td style=\"background-color:green;\">No</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td>Did the clerk understand the customer's inquiry?</td>\n",
       "    <td style=\"background-color:green;\">Yes</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td>Did the clerk provide clear and detailed information?</td>\n",
       "    <td style=\"background-color:green;\">Yes</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td>Did the clerk ask questions to clarify the situation?</td>\n",
       "    <td style=\"background-color:green;\">Yes</"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "llm=init_llm()\n",
    "prompt_prefix = \"\"\"Please compile a summary of the following information for the attention of the \n",
    "call center supervisor.\n",
    "Present the output in the form of an HTML table, with each item on a separate row.\n",
    "\n",
    "Assign a color code to each item based on the clerk's performance - \n",
    "items that were successfully addressed should be marked in green, \n",
    "whereas items that were not met should be highlighted in red.\n",
    "\n",
    "Here are the evaluation criteria to consider:\n",
    "\n",
    "Did the clerk greet the customer in a polite and professional manner?\n",
    "Did the clerk began the conversation on a negative note?\n",
    "Did the clerk understand the customer's inquiry?\n",
    "Did the clerk provide clear and detailed information?\n",
    "Did the clerk ask questions to clarify the situation?\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "res = summarize_text(llm, prompt_prefix, \"./data/bank-call-center-transcript.txt\")\n",
    "display(HTML(res.strip()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Output as JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens in final prompt is: 847\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "Ask Clarifying Questions": "Yes",
       "Discuss the Benefits and Risks": "Yes",
       "End on a Positive Note": "Yes",
       "Explain Available Tools and Resources": "Yes",
       "Greet the Customer Politely and Professionally": "Yes",
       "Invite Further Action": "Yes",
       "Offer to Assist with Next Steps": "Yes",
       "Provide Clear and Detailed Information": "Yes",
       "Understand the Customer's Inquiry": "Yes"
      },
      "text/plain": [
       "<IPython.core.display.JSON object>"
      ]
     },
     "metadata": {
      "application/json": {
       "expanded": false,
       "root": "root"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "llm=init_llm()\n",
    "prompt_prefix = \"\"\"\"Prepare a summary of the following points for the call center supervisor. \n",
    "The output should be presented in JSON format, adhering to the following schema:\n",
    "\n",
    "{\n",
    "    \"Greet the Customer Politely and Professionally\": \"Yes/No\",\n",
    "    \"Understand the Customer's Inquiry\": \"Yes/No\",\n",
    "    \"Provide Clear and Detailed Information\": \"Yes/No\",\n",
    "    \"Ask Clarifying Questions\": \"Yes/No\",\n",
    "    \"Discuss the Benefits and Risks\": \"Yes/No\",\n",
    "    \"Explain Available Tools and Resources\": \"Yes/No\",\n",
    "    \"Invite Further Action\": \"Yes/No\",\n",
    "    \"Offer to Assist with Next Steps\": \"Yes/No\",\n",
    "    \"End on a Positive Note\": \"Yes/No\"\n",
    "}\n",
    "\n",
    "Please evaluate the clerk's performance based on the following points:\n",
    "\n",
    "Did the clerk greet the customer in a polite and professional manner?\n",
    "Did the clerk comprehend the customer's inquiry accurately?\n",
    "Did the clerk provide comprehensive and clear information?\n",
    "Did the clerk ask relevant questions to clarify the customer's situation?\n",
    "Did the clerk explain the benefits and potential risks to the customer?\n",
    "Did the clerk detail the tools and resources available to the customer?\n",
    "Did the clerk encourage the customer to take further action?\n",
    "Did the clerk offer assistance with proceeding to the next steps?\n",
    "Did the clerk end the interaction on a positive and upbeat note?\"\n",
    "\"\"\"\n",
    "\n",
    "res = summarize_text(llm, prompt_prefix, \"./data/bank-call-center-transcript.txt\")\n",
    "display(JSON(json.loads(res)))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarize large documents\n",
    "To summarize larger documents, we can split the document into smaller chunks and summarize each chunk separately. We can then combine the summaries of each chunk to get the final summary.\n",
    "LangChain has a built-in chain for doing that. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import OpenAI\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected string or buffer",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[98], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdocument_loaders\u001b[39;00m \u001b[39mimport\u001b[39;00m PyPDFLoader\n\u001b[1;32m      2\u001b[0m llm\u001b[39m=\u001b[39minit_llm()\n\u001b[0;32m----> 3\u001b[0m llm\u001b[39m.\u001b[39;49mget_num_tokens(pages)\n\u001b[1;32m      4\u001b[0m large_pdf_path \u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m./data/Large_language_model.pdf\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      6\u001b[0m loader \u001b[39m=\u001b[39m PyPDFLoader(large_pdf_path)\n",
      "File \u001b[0;32m~/myprojects/openai/workshops/dataai/openaiworkshop/.venv/lib/python3.10/site-packages/langchain/base_language.py:77\u001b[0m, in \u001b[0;36mBaseLanguageModel.get_num_tokens\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_num_tokens\u001b[39m(\u001b[39mself\u001b[39m, text: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mint\u001b[39m:\n\u001b[1;32m     76\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Get the number of tokens present in the text.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 77\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_token_ids(text))\n",
      "File \u001b[0;32m~/myprojects/openai/workshops/dataai/openaiworkshop/.venv/lib/python3.10/site-packages/langchain/llms/openai.py:483\u001b[0m, in \u001b[0;36mBaseOpenAI.get_token_ids\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    475\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mImportError\u001b[39;00m(\n\u001b[1;32m    476\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mCould not import tiktoken python package. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    477\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mThis is needed in order to calculate get_num_tokens. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    478\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mPlease install it with `pip install tiktoken`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    479\u001b[0m     )\n\u001b[1;32m    481\u001b[0m enc \u001b[39m=\u001b[39m tiktoken\u001b[39m.\u001b[39mencoding_for_model(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_name)\n\u001b[0;32m--> 483\u001b[0m \u001b[39mreturn\u001b[39;00m enc\u001b[39m.\u001b[39;49mencode(\n\u001b[1;32m    484\u001b[0m     text,\n\u001b[1;32m    485\u001b[0m     allowed_special\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mallowed_special,\n\u001b[1;32m    486\u001b[0m     disallowed_special\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdisallowed_special,\n\u001b[1;32m    487\u001b[0m )\n",
      "File \u001b[0;32m~/myprojects/openai/workshops/dataai/openaiworkshop/.venv/lib/python3.10/site-packages/tiktoken/core.py:116\u001b[0m, in \u001b[0;36mEncoding.encode\u001b[0;34m(self, text, allowed_special, disallowed_special)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(disallowed_special, \u001b[39mfrozenset\u001b[39m):\n\u001b[1;32m    115\u001b[0m         disallowed_special \u001b[39m=\u001b[39m \u001b[39mfrozenset\u001b[39m(disallowed_special)\n\u001b[0;32m--> 116\u001b[0m     \u001b[39mif\u001b[39;00m match \u001b[39m:=\u001b[39m _special_token_regex(disallowed_special)\u001b[39m.\u001b[39;49msearch(text):\n\u001b[1;32m    117\u001b[0m         raise_disallowed_special_token(match\u001b[39m.\u001b[39mgroup())\n\u001b[1;32m    119\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "\u001b[0;31mTypeError\u001b[0m: expected string or buffer"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "llm=init_llm()\n",
    "llm.get_num_tokens(pages)\n",
    "large_pdf_path =\"./data/Large_language_model.pdf\"\n",
    "\n",
    "loader = PyPDFLoader(large_pdf_path)\n",
    "#the output type is List[Document]\n",
    "pages = loader.load_and_split()\n",
    "\n",
    "#count tokens\n",
    "total_tokens = 0\n",
    "\n",
    "for page in pages:\n",
    "    total_tokens += llm.get_num_tokens(page.page_content)\n",
    "    \n",
    "print(f\"Total tokens in the document: {total_tokens}\")\n",
    "\n",
    "#llm.get_num_tokens(entire_pdf.)\n",
    "\n",
    "\n",
    "#print(pages[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected string or buffer",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[95], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m llm\u001b[39m=\u001b[39minit_llm()\n\u001b[0;32m----> 2\u001b[0m llm\u001b[39m.\u001b[39;49mget_num_tokens(pages)\n",
      "File \u001b[0;32m~/myprojects/openai/workshops/dataai/openaiworkshop/.venv/lib/python3.10/site-packages/langchain/base_language.py:77\u001b[0m, in \u001b[0;36mBaseLanguageModel.get_num_tokens\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_num_tokens\u001b[39m(\u001b[39mself\u001b[39m, text: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mint\u001b[39m:\n\u001b[1;32m     76\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Get the number of tokens present in the text.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 77\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_token_ids(text))\n",
      "File \u001b[0;32m~/myprojects/openai/workshops/dataai/openaiworkshop/.venv/lib/python3.10/site-packages/langchain/llms/openai.py:483\u001b[0m, in \u001b[0;36mBaseOpenAI.get_token_ids\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    475\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mImportError\u001b[39;00m(\n\u001b[1;32m    476\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mCould not import tiktoken python package. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    477\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mThis is needed in order to calculate get_num_tokens. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    478\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mPlease install it with `pip install tiktoken`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    479\u001b[0m     )\n\u001b[1;32m    481\u001b[0m enc \u001b[39m=\u001b[39m tiktoken\u001b[39m.\u001b[39mencoding_for_model(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_name)\n\u001b[0;32m--> 483\u001b[0m \u001b[39mreturn\u001b[39;00m enc\u001b[39m.\u001b[39;49mencode(\n\u001b[1;32m    484\u001b[0m     text,\n\u001b[1;32m    485\u001b[0m     allowed_special\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mallowed_special,\n\u001b[1;32m    486\u001b[0m     disallowed_special\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdisallowed_special,\n\u001b[1;32m    487\u001b[0m )\n",
      "File \u001b[0;32m~/myprojects/openai/workshops/dataai/openaiworkshop/.venv/lib/python3.10/site-packages/tiktoken/core.py:116\u001b[0m, in \u001b[0;36mEncoding.encode\u001b[0;34m(self, text, allowed_special, disallowed_special)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(disallowed_special, \u001b[39mfrozenset\u001b[39m):\n\u001b[1;32m    115\u001b[0m         disallowed_special \u001b[39m=\u001b[39m \u001b[39mfrozenset\u001b[39m(disallowed_special)\n\u001b[0;32m--> 116\u001b[0m     \u001b[39mif\u001b[39;00m match \u001b[39m:=\u001b[39m _special_token_regex(disallowed_special)\u001b[39m.\u001b[39;49msearch(text):\n\u001b[1;32m    117\u001b[0m         raise_disallowed_special_token(match\u001b[39m.\u001b[39mgroup())\n\u001b[1;32m    119\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "\u001b[0;31mTypeError\u001b[0m: expected string or buffer"
     ]
    }
   ],
   "source": [
    "llm=init_llm()\n",
    "llm.get_num_tokens(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
